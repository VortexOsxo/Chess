{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VortexOsxo/Chess/blob/master/data_transform/pca-svd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erz4XXhBP2Ar",
        "outputId": "841e69a1-526b-41e5-99a5-a80211c50590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/Inf8245/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaD1Az-9JriJ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UgJKQwXYP4VZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_meta_data():\n",
        "    meta_train = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    meta_test = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    pass\n",
        "\n",
        "def load_data():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    X_train, y_train = data_train[\"X_train\"], data_train[\"y_train\"] # data_train[\"ids\"]\n",
        "    X_test = data_test[\"X_test\"] # data_test[\"ids\"]\n",
        "\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "def merge_train_test(X_train, X_test, debug=False):\n",
        "    X_combined = np.concatenate([X_train, X_test], axis=0)\n",
        "    if debug: print(X_combined.shape)\n",
        "    return X_combined\n",
        "\n",
        "def remove_null_variance_column(X_train, X_test, debug=False):\n",
        "    mask = (X_train.min(axis=0) != X_train.max(axis=0))\n",
        "\n",
        "    if debug: print(f'Initial Column number: {X_train.shape[1]}')\n",
        "    X_train = X_train[:, mask]\n",
        "    X_test  = X_test[:, mask]\n",
        "    if debug: print(f'After removed null variance: {X_train.shape[1]}')\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def StandardNormalization(X_train, X_test):\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    return scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "def MinMaxNormalization(X_train, X_test):\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    return scaler.transform(X_train), scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQK77YJzAq_"
      },
      "source": [
        "### PCA\n",
        "\n",
        "Run incremental PCA epochs until the improvement on explained variance is lower than `tolerance`, for a max of `max_epochs`, with a batch_size of `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zUK7bgU4QQTT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import defaultdict\n",
        "\n",
        "def incremental_pca(data, n_components, tolerance=1e-4, max_epochs=10, batch_size=500, debug=True):\n",
        "  ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "  n_samples = len(X_train)\n",
        "  best_ipca = None\n",
        "  min_variance_diff = float('inf')\n",
        "\n",
        "  if debug: print(f\"Starting Incremental PCA with {max_epochs} epochs and tolerance {tolerance}\")\n",
        "\n",
        "  for epoch in range(1, max_epochs + 1):\n",
        "      if debug: print(f\"Epoch {epoch}: Starting...\")\n",
        "\n",
        "      shuffled_indices = np.random.permutation(n_samples)\n",
        "      X_all_shuffled = data[shuffled_indices]\n",
        "\n",
        "      current_epoch_variance_ratios = []\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "          batch = X_all_shuffled[i:i + batch_size]\n",
        "          ipca.partial_fit(batch)\n",
        "\n",
        "          if ipca.explained_variance_ratio_ is not None:\n",
        "              current_epoch_variance_ratios.append(ipca.explained_variance_ratio_.sum())\n",
        "\n",
        "      if epoch > 1:\n",
        "          prev_variance_sum = current_epoch_variance_ratios_prev[-1]\n",
        "          current_variance_sum = current_epoch_variance_ratios[-1]\n",
        "\n",
        "          variance_diff = abs(current_variance_sum - prev_variance_sum)\n",
        "\n",
        "          if debug: print(f\"Epoch {epoch} finished. Total Explained Variance: {current_variance_sum:.4f}\")\n",
        "          if debug: print(f\"Variance change from last epoch: {variance_diff:.6f}\")\n",
        "\n",
        "          if variance_diff < tolerance:\n",
        "              if debug: print(\"\\n **Converged!** Variance change is below tolerance.\")\n",
        "              break\n",
        "\n",
        "          # Optional: Save the best model based on minimum variance change\n",
        "          if variance_diff < min_variance_diff:\n",
        "              min_variance_diff = variance_diff\n",
        "              best_ipca = ipca\n",
        "\n",
        "      else:\n",
        "          print(f\"Epoch {epoch} finished. Total Explained Variance: {current_epoch_variance_ratios[-1]:.4f}\")\n",
        "      current_epoch_variance_ratios_prev = current_epoch_variance_ratios\n",
        "\n",
        "  else:\n",
        "      print(\"\\n **Stopped!** Maximum number of epochs reached without convergence.\")\n",
        "\n",
        "  return ipca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUtT4-M_eIh"
      },
      "source": [
        "### SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "engG0wnC_f0i"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def truncated_svd(data, n_components, tolerance=1e-5):\n",
        "  svd = TruncatedSVD(n_components=n_components, algorithm='arpack', tol=tolerance)\n",
        "  svd.fit(data)\n",
        "  return svd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXWKxBgJjWU"
      },
      "source": [
        "### Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Im6hrBBr6Bva"
      },
      "outputs": [],
      "source": [
        "def save_transform(ipca, X_train, X_test, transform, name):\n",
        "  X_train_pca = ipca.transform(X_train)\n",
        "  X_test_pca = ipca.transform(X_test)\n",
        "\n",
        "  np.save(f'{file_path}Data/{transform}/{name}train.npy', X_train_pca)\n",
        "  np.save(f'{file_path}Data/{transform}/{name}test.npy', X_test_pca)\n",
        "\n",
        "def load_transform(transform, n_components, is_combined=True, tolerance=1e-4):\n",
        "  name = f'nc_{n_components}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "  X_train = np.load(f'{file_path}Data/{transform}/{name}train.npy')\n",
        "  X_test = np.load(f'{file_path}Data/{transform}/{name}test.npy')\n",
        "  return X_train, X_test\n",
        "\n",
        "def load_pca(n_components, combined, tolerance=1e-4):\n",
        "  return load_transform('PCA', n_components, combined, tolerance)\n",
        "\n",
        "def load_sdv(n_components, combined, tolerance=1e-4):\n",
        "  return load_transform('SVD', n_components, combined, tolerance)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YwrKKTBJk9s"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kYKwct5I99L"
      },
      "outputs": [],
      "source": [
        "is_combined = True\n",
        "n_components = [] # 64, 96, 128, 192, 256, 512, 1024  Max seems to be ~600 with 51GB RAM (for PCA)\n",
        "tolerance = 1e-5\n",
        "batch_size = 600\n",
        "\n",
        "transform = 'SVD'\n",
        "if __name__ == '__main__':\n",
        "    X_train, y_train, X_test = load_data()\n",
        "    X_train, X_test = remove_null_variance_column(X_train, X_test)\n",
        "\n",
        "    data = merge_train_test(X_train, X_test) if is_combined == 'true' else X_train\n",
        "    for n in n_components:\n",
        "        name = f'nc_{n}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "        if transform == 'PCA':\n",
        "          model = incremental_pca(data, n, tolerance, batch_size=batch_size)\n",
        "        elif transform == 'SVD':\n",
        "          model = truncated_svd(data, n, tolerance)\n",
        "        else:\n",
        "          raise 1\n",
        "\n",
        "        save_transform(model, X_train, X_test, transform, name)\n",
        "        print(f'Saved: {name}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN4Y5QMYIvYNVKhCC5HacbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}