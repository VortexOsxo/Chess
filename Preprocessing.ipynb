{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erz4XXhBP2Ar",
        "outputId": "347938d3-6e08-4151-b9cf-9d811fb7c6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/Inf8245/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "import os\n",
        "\n",
        "# Setting this environment variable fixes the CuBLAS non-determinism issue\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        try:\n",
        "            torch.use_deterministic_algorithms(True)\n",
        "        except:\n",
        "            print(\"Note: torch.use_deterministic_algorithms(True) failed. Check PyTorch version.\")\n",
        "\n",
        "    print(f\"Global seed set to {seed_value}.\")\n",
        "set_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L9zEKIenO7l",
        "outputId": "3c816d91-e1eb-43be-8a9f-bd91d73ef88c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 42.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaD1Az-9JriJ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UgJKQwXYP4VZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_meta_data():\n",
        "    meta_train = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    meta_test = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    pass\n",
        "\n",
        "def load_data():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    X_train, y_train = data_train[\"X_train\"], data_train[\"y_train\"] # data_train[\"ids\"]\n",
        "    X_test = data_test[\"X_test\"] # data_test[\"ids\"]\n",
        "\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "def merge_train_test(X_train, X_test, debug=False):\n",
        "    X_combined = np.concatenate([X_train, X_test], axis=0)\n",
        "    if debug: print(X_combined.shape)\n",
        "    return X_combined\n",
        "\n",
        "def remove_null_variance_column(X_train, X_test, debug=False):\n",
        "    mask = (X_train.min(axis=0) != X_train.max(axis=0))\n",
        "\n",
        "    if debug: print(f'Initial Column number: {X_train.shape[1]}')\n",
        "    X_train = X_train[:, mask]\n",
        "    X_test  = X_test[:, mask]\n",
        "    if debug: print(f'After removed null variance: {X_train.shape[1]}')\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def StandardNormalization(X_train, X_test):\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    return scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "def MinMaxNormalization(X_train, X_test):\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    return scaler.transform(X_train), scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQK77YJzAq_"
      },
      "source": [
        "### PCA\n",
        "\n",
        "Run incremental PCA epochs until the improvement on explained variance is lower than `tolerance`, for a max of `max_epochs`, with a batch_size of `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zUK7bgU4QQTT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import defaultdict\n",
        "\n",
        "def incremental_pca(data, n_components, tolerance=1e-4, max_epochs=10, batch_size=500, debug=True):\n",
        "  ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "  n_samples = len(X_train)\n",
        "  best_ipca = None\n",
        "  min_variance_diff = float('inf')\n",
        "\n",
        "  if debug: print(f\"Starting Incremental PCA with {max_epochs} epochs and tolerance {tolerance}\")\n",
        "\n",
        "  for epoch in range(1, max_epochs + 1):\n",
        "      if debug: print(f\"Epoch {epoch}: Starting...\")\n",
        "\n",
        "      shuffled_indices = np.random.permutation(n_samples)\n",
        "      X_all_shuffled = data[shuffled_indices]\n",
        "\n",
        "      current_epoch_variance_ratios = []\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "          batch = X_all_shuffled[i:i + batch_size]\n",
        "          ipca.partial_fit(batch)\n",
        "\n",
        "          if ipca.explained_variance_ratio_ is not None:\n",
        "              current_epoch_variance_ratios.append(ipca.explained_variance_ratio_.sum())\n",
        "\n",
        "      if epoch > 1:\n",
        "          prev_variance_sum = current_epoch_variance_ratios_prev[-1]\n",
        "          current_variance_sum = current_epoch_variance_ratios[-1]\n",
        "\n",
        "          variance_diff = abs(current_variance_sum - prev_variance_sum)\n",
        "\n",
        "          if debug: print(f\"Epoch {epoch} finished. Total Explained Variance: {current_variance_sum:.4f}\")\n",
        "          if debug: print(f\"Variance change from last epoch: {variance_diff:.6f}\")\n",
        "\n",
        "          if variance_diff < tolerance:\n",
        "              if debug: print(\"\\n **Converged!** Variance change is below tolerance.\")\n",
        "              break\n",
        "\n",
        "          # Optional: Save the best model based on minimum variance change\n",
        "          if variance_diff < min_variance_diff:\n",
        "              min_variance_diff = variance_diff\n",
        "              best_ipca = ipca\n",
        "\n",
        "      else:\n",
        "          print(f\"Epoch {epoch} finished. Total Explained Variance: {current_epoch_variance_ratios[-1]:.4f}\")\n",
        "      current_epoch_variance_ratios_prev = current_epoch_variance_ratios\n",
        "\n",
        "  else:\n",
        "      print(\"\\n **Stopped!** Maximum number of epochs reached without convergence.\")\n",
        "\n",
        "  return ipca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUtT4-M_eIh"
      },
      "source": [
        "### SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "engG0wnC_f0i"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def truncated_svd(data, n_components, tolerance=1e-5):\n",
        "  svd = TruncatedSVD(n_components=n_components, algorithm='arpack', tol=tolerance)\n",
        "  svd.fit(data)\n",
        "  return svd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder\n"
      ],
      "metadata": {
        "id": "k9gUvSlLlDN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class LinearAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, n_components):\n",
        "        super(LinearAutoencoder, self).__init__()\n",
        "        # Encoder: Maps input to latent space (similar to PCA components)\n",
        "        self.encoder = nn.Linear(input_dim, n_components, bias=True)\n",
        "        # Decoder: Maps latent space back to input\n",
        "        self.decoder = nn.Linear(n_components, input_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def transform(self, X):\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "        with torch.no_grad():\n",
        "            # Convert numpy to tensor\n",
        "            X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            # Encode\n",
        "            encoded = self.encoder(X_tensor)\n",
        "            # Return to numpy\n",
        "            return encoded.cpu().numpy()\n",
        "\n",
        "def train_linear_ae(data, n_components, epochs=100, batch_size=500, lr=1e-3, tolerance=1e-5, patience=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Training on {device}...\")\n",
        "\n",
        "    input_dim = data.shape[1]\n",
        "    model = LinearAutoencoder(input_dim, n_components).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Data Loader\n",
        "    tensor_x = torch.tensor(data, dtype=torch.float32)\n",
        "    dataset = TensorDataset(tensor_x, tensor_x)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Tracking\n",
        "    loss_history = []\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_x, _ in dataloader:\n",
        "            batch_x = batch_x.to(device)\n",
        "\n",
        "            # Forward & Backward\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_x)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Average loss for this epoch\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        # --- Convergence Check (Early Stopping) ---\n",
        "        # If the loss improved by more than 'tolerance'\n",
        "        if avg_loss < best_loss - tolerance:\n",
        "            best_loss = avg_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # Stop if no improvement for 'patience' epochs\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Converged at Epoch {epoch+1} with Loss: {avg_loss:.6f}\")\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')\n",
        "\n",
        "    # --- Visualization ---\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(loss_history, label='Reconstruction Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title(f'Convergence: {n_components} Components')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show() # This will display the graph in your notebook\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XXZq44gKlEqa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXWKxBgJjWU"
      },
      "source": [
        "### Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Im6hrBBr6Bva"
      },
      "outputs": [],
      "source": [
        "def save_transform(ipca, X_train, X_test, transform, name):\n",
        "  X_train_pca = ipca.transform(X_train)\n",
        "  X_test_pca = ipca.transform(X_test)\n",
        "\n",
        "  np.save(f'{file_path}Data/{transform}/{name}train.npy', X_train_pca)\n",
        "  np.save(f'{file_path}Data/{transform}/{name}test.npy', X_test_pca)\n",
        "\n",
        "def load_transform(transform, n_components, is_combined=True, tolerance=1e-4):\n",
        "  name = f'nc_{n_components}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "  X_train = np.load(f'{file_path}Data/{transform}/{name}train.npy')\n",
        "  X_test = np.load(f'{file_path}Data/{transform}/{name}test.npy')\n",
        "  return X_train, X_test\n",
        "\n",
        "def load_pca(n_components, combined, tolerance=1e-4):\n",
        "  return load_transform('PCA', n_components, combined, tolerance)\n",
        "\n",
        "def load_sdv(n_components, combined, tolerance=1e-4):\n",
        "  return load_transform('SVD', n_components, combined, tolerance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YwrKKTBJk9s"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kYKwct5I99L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c2e7bb-1e4c-4047-af2d-994853472202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda...\n",
            "Epoch [5/100], Loss: 111.021280\n",
            "Epoch [10/100], Loss: 1.296625\n",
            "Epoch [15/100], Loss: 1.054853\n",
            "Epoch [20/100], Loss: 1.044140\n",
            "Epoch [25/100], Loss: 1.035530\n",
            "Epoch [30/100], Loss: 1.027152\n",
            "Epoch [35/100], Loss: 1.018735\n",
            "Epoch [40/100], Loss: 1.010402\n",
            "Epoch [45/100], Loss: 1.001764\n",
            "Epoch [50/100], Loss: 0.992828\n",
            "Epoch [55/100], Loss: 0.984007\n",
            "Epoch [60/100], Loss: 0.974281\n",
            "Epoch [65/100], Loss: 0.964197\n",
            "Epoch [70/100], Loss: 0.954350\n",
            "Epoch [75/100], Loss: 0.944162\n"
          ]
        }
      ],
      "source": [
        "is_combined = True\n",
        "n_components = [96, 128, 192, 256, 512,] # 64, 96, 128, 192, 256, 512, 1024  Max seems to be ~600 with 51GB RAM (for PCA)\n",
        "tolerance = 1e-5\n",
        "batch_size = 256\n",
        "\n",
        "transform = 'AE'\n",
        "if __name__ == '__main__':\n",
        "    X_train, y_train, X_test = load_data()\n",
        "    X_train, X_test = remove_null_variance_column(X_train, X_test)\n",
        "\n",
        "    data = merge_train_test(X_train, X_test) if is_combined else X_train\n",
        "    for n in n_components:\n",
        "        name = f'nc_{n}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "\n",
        "        if transform == 'PCA':\n",
        "            model = incremental_pca(data, n, tolerance, batch_size=batch_size)\n",
        "        elif transform == 'SVD':\n",
        "            model = truncated_svd(data, n, tolerance)\n",
        "        elif transform == 'AE':\n",
        "            model = train_linear_ae(data, n, epochs=100, batch_size=batch_size)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown transform type\")\n",
        "\n",
        "        save_transform(model, X_train, X_test, transform, name)\n",
        "        print(f'Saved: {name}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOAOS8ZjrNKzx9UwUAh+zhl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}