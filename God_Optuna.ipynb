{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VortexOsxo/Chess/blob/master/God_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDcP1v2sLxN"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k8x2miYsRpM",
        "outputId": "024c33f5-4215-4b95-ef3c-c4248a29dcd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/Inf8245/'\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "NUM_FOLDS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "P1qOaetVsOQu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_meta_data():\n",
        "    meta_train = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    meta_test = pd.read_csv(f'{file_path}Data/metadata_test.csv')\n",
        "    return meta_train, meta_test\n",
        "\n",
        "def load_data():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    X_train, y_train = data_train[\"X_train\"], data_train[\"y_train\"]\n",
        "    X_test = data_test[\"X_test\"]\n",
        "\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "def load_ids():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    return data_train[\"ids\"], data_test[\"ids\"]\n",
        "\n",
        "def save_predictions(preds, filename):\n",
        "    _, test_ids = load_ids()\n",
        "\n",
        "    if len(test_ids) != len(preds):\n",
        "        raise ValueError(\"Length of test_ids and preds must be the same.\")\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"id\": test_ids.astype(str),\n",
        "        \"label\": preds\n",
        "    })\n",
        "\n",
        "    df.to_csv(f'{file_path}Predictions/{filename}.csv', index=False)\n",
        "\n",
        "def merge_train_test(X_train, X_test, debug=False):\n",
        "    X_combined = np.concatenate([X_train, X_test], axis=0)\n",
        "    if debug: print(X_combined.shape)\n",
        "    return X_combined\n",
        "\n",
        "def remove_null_variance_column(X_train, X_test, debug=False):\n",
        "    mask = (X_train.min(axis=0) != X_train.max(axis=0))\n",
        "\n",
        "    if debug: print(f'Initial Column number: {X_train.shape[1]}')\n",
        "    X_train = X_train[:, mask]\n",
        "    X_test  = X_test[:, mask]\n",
        "    if debug: print(f'After removed null variance: {X_train.shape[1]}')\n",
        "\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-Jc-wNytx4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxUsQWG-y0OJ"
      },
      "source": [
        "## PCA & SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "gNA4otlBteYM"
      },
      "outputs": [],
      "source": [
        "def load_transform(transform, n_components, is_combined=True, tolerance=1e-5):\n",
        "  name = f'nc_{n_components}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "  X_train = np.load(f'{file_path}Data/{transform}/{name}train.npy')\n",
        "  X_test = np.load(f'{file_path}Data/{transform}/{name}test.npy')\n",
        "  return X_train, X_test\n",
        "\n",
        "def load_pca(n_components, is_combined=True, tolerance=1e-5):\n",
        "  return load_transform('PCA', n_components, is_combined, tolerance)\n",
        "\n",
        "def load_svd(n_components, is_combined=True, tolerance=1e-5):\n",
        "  return load_transform('SVD', n_components, is_combined, tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOganBvY2I-j"
      },
      "source": [
        "## Chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "x_pqcwUn7eqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
        "\n",
        "def load_var_chi2(X_train, X_test, y_train, n_components, threshold):\n",
        "  selector = VarianceThreshold(threshold=threshold)\n",
        "  X_train  =  selector.fit_transform(X_train)\n",
        "  X_test   = selector.transform(X_test)\n",
        "\n",
        "  selector = SelectKBest(chi2, k=n_components)\n",
        "\n",
        "  X_train = selector.fit_transform(X_train, y_train)\n",
        "  X_test  = selector.transform(X_test)\n",
        "\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBAoAxnH-wk_"
      },
      "source": [
        "## Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "nWCX8kGn-zmV"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def save_metadata(meta_train, meta_test, reduced=False):\n",
        "  joblib.dump(meta_train, f\"{file_path}Data/X_meta_train_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "  joblib.dump(meta_test,  f\"{file_path}Data/X_meta_test_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "\n",
        "def load_meta_onehot(reduced=False, debug=False):\n",
        "  X_meta_train = joblib.load(f\"{file_path}Data/X_meta_train_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "  X_meta_test  = joblib.load(f\"{file_path}Data/X_meta_test_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "\n",
        "  X_meta_train_dense = X_meta_train.toarray() if sparse.issparse(X_meta_train) else X_meta_train\n",
        "  X_meta_test_dense  = X_meta_test.toarray()  if sparse.issparse(X_meta_test)  else X_meta_test\n",
        "\n",
        "  return X_meta_train_dense, X_meta_test_dense\n",
        "\n",
        "def append_meta_to_data(X_train, X_test, reduced=False, debug=False):\n",
        "  meta_train, meta_test = load_meta_onehot(reduced)\n",
        "\n",
        "  X_train_combined = np.hstack([meta_train, X_train])\n",
        "  X_test_combined  = np.hstack([meta_test,  X_test])\n",
        "\n",
        "  return X_train_combined, X_test_combined\n",
        "\n",
        "def build_categorical_features(meta_train, meta_test, selected_columns):\n",
        "  if len(selected_columns) == 0:\n",
        "    # No metadata selected by Optuna\n",
        "    n_train = meta_train.shape[0]\n",
        "    n_test = meta_test.shape[0]\n",
        "    return np.zeros((n_train, 0)), np.zeros((n_test, 0))\n",
        "\n",
        "  train_cat = meta_train[selected_columns].copy()\n",
        "  test_cat = meta_test[selected_columns].copy()\n",
        "\n",
        "  threshold = 15\n",
        "\n",
        "  for col in selected_columns:\n",
        "    counts = train_cat[col].value_counts(dropna=False)\n",
        "    keep_categories = set(counts[counts >= threshold].index)\n",
        "\n",
        "    def map_value(v):\n",
        "      return v if v in keep_categories else \"Other\"\n",
        "\n",
        "    train_cat[col] = train_cat[col].map(map_value)\n",
        "    test_cat[col] = test_cat[col].map(map_value)\n",
        "\n",
        "  encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "  train_encoded = encoder.fit_transform(train_cat)\n",
        "  test_encoded = encoder.transform(test_cat)\n",
        "\n",
        "  return train_encoded, test_encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ocuFbltfFv"
      },
      "source": [
        "# Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjGkyVozucKG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "h-YR7Mtvudh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d125cff-78a8-4183-b152-de52313dcd11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr00udAc0dLN"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KxEiSYcO0ecD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6705b72-6875-49e8-a4e4-bcd23f9e12f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [OK] Loaded input_size=64  | Shape: (1939, 64)\n",
            "  [OK] Loaded input_size=96  | Shape: (1939, 96)\n",
            "  [OK] Loaded input_size=128 | Shape: (1939, 128)\n",
            "  [OK] Loaded input_size=192 | Shape: (1939, 192)\n",
            "  [OK] Loaded input_size=256 | Shape: (1939, 256)\n",
            "  [OK] Loaded input_size=512 | Shape: (1939, 512)\n"
          ]
        }
      ],
      "source": [
        "SIZES = [64, 96, 128, 192, 256, 512]\n",
        "\n",
        "\n",
        "class MyDataLoader():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.meta_train, self.meta_test = load_meta_data()\n",
        "        self.X_train, self.y_train, self.X_test = load_data()\n",
        "        self.data_dict = {\n",
        "            'PCA': {}, 'SVD':{},\n",
        "        }\n",
        "        self.load_data()\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        for size in SIZES:\n",
        "            X_tr, _ = load_svd(n_components=size)\n",
        "            self.data_dict['SVD'][size] = X_tr\n",
        "\n",
        "            X_tr, _ = load_pca(n_components=size)\n",
        "            self.data_dict['PCA'][size] = X_tr\n",
        "            print(f\"  [OK] Loaded input_size={size:<3} | Shape: {X_tr.shape}\")\n",
        "\n",
        "\n",
        "    def get_combined_data(self, config, train, test=None) -> np.ndarray:\n",
        "        meta_train, meta_test = build_categorical_features(\n",
        "            self.meta_train, self.meta_test,\n",
        "            config['selected_columns'],\n",
        "        )\n",
        "        if test is None:\n",
        "            return np.hstack([meta_train, train])\n",
        "        return np.hstack([meta_train, train]), np.hstack([meta_test, test])\n",
        "\n",
        "\n",
        "    def get_val_data(self, type, config):\n",
        "        if type in ['PCA', 'SVD']:\n",
        "            X_train = self.data_dict[type][config['input_size']]\n",
        "        elif type == 'CHI2':\n",
        "            X_train, _ = load_var_chi2(self.X_train, self.X_test, self.y_train, config['input_size'], config['chi2_threshold'])\n",
        "        return self.get_combined_data(config, X_train)\n",
        "\n",
        "\n",
        "    def get_test_data(self, type, config):\n",
        "        if type == 'SVD':\n",
        "            X_train, X_test = load_svd(n_components=config['input_size'])\n",
        "        elif type == 'PCA':\n",
        "            X_train, X_test = load_pca(n_components=config['input_size'])\n",
        "        elif type == 'CHI2':\n",
        "            X_train, X_test = load_var_chi2(self.X_train, self.X_test, self.y_train, config['input_size'], config['chi2_threshold'])\n",
        "        return self.get_combined_data(config, X_train, X_test)\n",
        "\n",
        "\n",
        "data_loader = MyDataLoader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWnjEqHue5g"
      },
      "source": [
        "# Hyperparameter Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BaseModelClass"
      ],
      "metadata": {
        "id": "sE32dpvjHAcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractclassmethod\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import datetime\n",
        "\n",
        "class ModelClass():\n",
        "  @classmethod\n",
        "  def objective(cls, trial):\n",
        "    config = cls.create_data_config(trial)\n",
        "    return cls.evaluate_model(config)\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    pass\n",
        "\n",
        "  @classmethod\n",
        "  def create_data_config(cls, trial):\n",
        "    config = {\n",
        "        'data_type':    trial.suggest_categorical(\"data_type\", ['PCA', 'SVD', 'CHI2']),\n",
        "        'use_smote':    trial.suggest_categorical(\"use_smote\", [True, False]),\n",
        "        'threshold':    trial.suggest_float(\"threshold\", 0.1, 0.7),\n",
        "    }\n",
        "\n",
        "    # Metadata feature used\n",
        "    config['selected_columns'] = [\n",
        "        col for col in ['Isolation type', 'Location', 'Isolation source', 'Testing standard']\n",
        "        if trial.suggest_categorical(f\"use_{col}\", [True, False])\n",
        "    ]\n",
        "\n",
        "    # Conditionnal\n",
        "    config['chi2_threshold'] = None\n",
        "    if config['data_type'] == 'CHI2':\n",
        "      config['input_size'] = trial.suggest_int(\"input_size_chi2\", 64, 1024, log=True)\n",
        "      config['chi2_threshold'] = trial.suggest_float(\"chi2_threshold\", 0.001, 0.1)\n",
        "    else:\n",
        "      config['input_size'] = trial.suggest_categorical(\"input_size_pca_svd\", SIZES)\n",
        "\n",
        "    # Smote\n",
        "    config['smote_k'] = trial.suggest_int(\"smote_k\", 1, 10) if config['use_smote'] else None\n",
        "    config['smote_ratio'] = trial.suggest_float(\"smote_ratio\", 0.3, 1.0) if config['use_smote'] else None\n",
        "    return cls.update_data_config(config, trial)\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    pass\n",
        "\n",
        "  @classmethod\n",
        "  def evaluate_model(cls, config):\n",
        "    # Cross validation folds\n",
        "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    f1_scores = []\n",
        "\n",
        "    # Get Data\n",
        "    X_source = data_loader.get_val_data(config['data_type'], config)\n",
        "    y_source = data_loader.y_train\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_source, y_source)):\n",
        "        X_tr, X_val = X_source[train_idx], X_source[val_idx]\n",
        "        y_tr, y_val = y_source[train_idx], y_source[val_idx]\n",
        "\n",
        "        # --- SMOTE ---\n",
        "        if config['use_smote']:\n",
        "            try:\n",
        "                smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "            except ValueError:\n",
        "                print('Smote Failed...')\n",
        "                pass\n",
        "\n",
        "        # Function to evaluate a model, return its predictions\n",
        "        val_probs = cls.train_predict(X_tr, X_val, y_tr, config)\n",
        "\n",
        "        val_preds = (val_probs > config['threshold']).astype(int)\n",
        "        score = f1_score(y_val, val_preds, average='macro')\n",
        "        f1_scores.append(score)\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "  @classmethod\n",
        "  def predict(cls, config, proba=False, save=False):\n",
        "        X_train_full, X_test_final = data_loader.get_test_data(config['data_type'], config)\n",
        "        y_train_full = data_loader.y_train\n",
        "\n",
        "        # 2. --- SMOTE ---\n",
        "        if config['use_smote']:\n",
        "            print(\"Applying SMOTE to full training set...\")\n",
        "            try:\n",
        "                smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
        "            except ValueError:\n",
        "                print('Smote Failed on full set, proceeding without oversampling.')\n",
        "                pass\n",
        "\n",
        "        probs = cls.train_predict(X_train_full, X_test_final, y_train_full, config)\n",
        "        results = probs if proba else (probs > config['threshold']).astype(int)\n",
        "\n",
        "        if len(results.shape) > 1:\n",
        "             results = results.flatten()\n",
        "        if save:\n",
        "            model_name = cls.__name__.replace('Model', '')\n",
        "            date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "            pred_type = \"PROBS\" if proba else \"PREDS\"\n",
        "\n",
        "            filename = f\"Submission_{model_name}_{date_str}_{pred_type}\"\n",
        "            save_predictions(results, filename)\n",
        "            print(f\"Saved submission to {filename}.csv\")\n",
        "        return results"
      ],
      "metadata": {
        "id": "Nh_FXuD8HCsV"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctMhNe_rX_ZH"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "uuHWxZQbYBI2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class LogisticRegressionModel(ModelClass):\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- LR Model Params ---\n",
        "    config['C'] = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
        "    config['penalty'] = trial.suggest_categorical(\"penalty\", ['l1', 'l2'])\n",
        "\n",
        "    # Solver choice depends on the penalty (saga supports both l1 and l2)\n",
        "    config['solver'] = 'saga'\n",
        "    config['max_iter'] = trial.suggest_int(\"max_iter\", 5000, 10000)\n",
        "\n",
        "    # --- Validation Params ---\n",
        "    config['scale_data'] = trial.suggest_categorical(\"scale_data\", [True, False])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    if config['scale_data']:\n",
        "      scaler = StandardScaler()\n",
        "      X_tr = scaler.fit_transform(X_tr)\n",
        "      X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Initialize and Train the Model\n",
        "    model = LogisticRegression(\n",
        "      C=config['C'],\n",
        "      penalty=config['penalty'],\n",
        "      solver=config['solver'],\n",
        "      max_iter=config['max_iter'],\n",
        "      random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwbbxAwZj4nZ"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Tnww3UKej59v"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class RandomForestModel(ModelClass):\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- RF Model Params ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 5, 30, log=True)\n",
        "    config['criterion']         = trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss'])\n",
        "    config['min_samples_split'] = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    config['min_samples_leaf']  = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "    config['max_features']      = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', 0.5, 0.7, 1.0, None])\n",
        "    config['bootstrap']         = trial.suggest_categorical(\"bootstrap\", [True])\n",
        "    config['class_weight']      = trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
        "    config['max_samples']       = trial.suggest_categorical(\"max_samples\", [None, 0.5, 0.7, 0.9])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "      model = RandomForestClassifier(\n",
        "        n_estimators=config['n_estimators'],\n",
        "        max_depth=config['max_depth'],\n",
        "        criterion=config['criterion'],\n",
        "        min_samples_split=config['min_samples_split'],\n",
        "        min_samples_leaf=config['min_samples_leaf'],\n",
        "        max_features=config['max_features'],\n",
        "        bootstrap=config['bootstrap'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=config['class_weight'],\n",
        "        max_samples=config['max_samples'],\n",
        "        n_jobs=-1\n",
        "      )\n",
        "\n",
        "      model.fit(X_tr, y_tr)\n",
        "      return model.predict_proba(X_val)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extremly Random Forest\n"
      ],
      "metadata": {
        "id": "zJXvBuL_3-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# Note: You can reuse the Random Forest's update_data_config method\n",
        "# or copy it here, as the parameters are nearly identical.\n",
        "\n",
        "class ExtraTreesModel(ModelClass):\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- ET Model Params (Copied from Random Forest, they are compatible) ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 5, 30, log=True)\n",
        "    config['criterion']         = trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss'])\n",
        "    config['min_samples_split'] = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    config['min_samples_leaf']  = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "    config['max_features']      = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', 0.5, 0.7, 1.0, None])\n",
        "    config['bootstrap']         = trial.suggest_categorical(\"bootstrap\", [True])\n",
        "    config['class_weight']      = trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
        "    config['max_samples']       = trial.suggest_categorical(\"max_samples\", [None, 0.5, 0.7, 0.9])\n",
        "\n",
        "    # ExtraTrees specific parameter, usually kept at 0 or tuned\n",
        "    config['min_impurity_decrease'] = trial.suggest_float(\"min_impurity_decrease\", 0.0, 0.1)\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "      model = ExtraTreesClassifier(\n",
        "        n_estimators=config['n_estimators'],\n",
        "        max_depth=config['max_depth'],\n",
        "        criterion=config['criterion'],\n",
        "        min_samples_split=config['min_samples_split'],\n",
        "        min_samples_leaf=config['min_samples_leaf'],\n",
        "        max_features=config['max_features'],\n",
        "        bootstrap=config['bootstrap'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=config['class_weight'],\n",
        "        max_samples=config['max_samples'],\n",
        "        min_impurity_decrease=config['min_impurity_decrease'], # Added ET specific parameter\n",
        "        n_jobs=-1\n",
        "      )\n",
        "\n",
        "      model.fit(X_tr, y_tr)\n",
        "      return model.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "biV5MYQ_4BU-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "V9zf_FvOZisb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "class XGBoostModel(ModelClass):\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- XGBoost Model Params ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
        "    config['learning_rate']     = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 3, 15)\n",
        "    config['min_child_weight']  = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
        "    config['subsample']         = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
        "    config['colsample_bytree']  = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
        "    config['gamma']             = trial.suggest_float(\"gamma\", 0, 5)\n",
        "    config['reg_alpha']         = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True)\n",
        "    config['reg_lambda']        = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True)\n",
        "    config['balance_strategy']  = trial.suggest_categorical(\"balance_strategy\", [\"None\", \"Balanced\"])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    # Calculate scale_pos_weight dynamicallly if strategy is Balanced\n",
        "    scale_pos_weight = 1.0\n",
        "    if config['balance_strategy'] == \"Balanced\":\n",
        "      scale_pos_weight = (len(y_tr) - np.sum(y_tr)) / np.sum(y_tr)\n",
        "\n",
        "    # Initialize and Train XGBoost\n",
        "    model = XGBClassifier(\n",
        "      n_estimators=config['n_estimators'],\n",
        "      learning_rate=config['learning_rate'],\n",
        "      max_depth=config['max_depth'],\n",
        "      min_child_weight=config['min_child_weight'],\n",
        "      subsample=config['subsample'],\n",
        "      colsample_bytree=config['colsample_bytree'],\n",
        "      gamma=config['gamma'],\n",
        "      reg_alpha=config['reg_alpha'],\n",
        "      reg_lambda=config['reg_lambda'],\n",
        "      scale_pos_weight=scale_pos_weight,\n",
        "      random_state=RANDOM_STATE,\n",
        "      n_jobs=-1,\n",
        "      tree_method='hist',\n",
        "      objective='binary:logistic',\n",
        "      eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "ukJz6bbDZkA0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "FakbV7Im3_1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class KnnModel(ModelClass):\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "      # --- KNN Model Params ---\n",
        "      config['n_neighbors']  = trial.suggest_int(\"n_neighbors\", 3, 100)\n",
        "      config['weights']      = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n",
        "      config['metric']       = trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'cosine', 'minkowski'])\n",
        "      config['algorithm']    = 'auto'\n",
        "\n",
        "      # --- Validation Params ---\n",
        "      config['scale_data']   = trial.suggest_categorical(\"scale_data\", [True, False])\n",
        "\n",
        "      # Conditional params for minkowski\n",
        "      config['p'] = trial.suggest_int(\"p\", 1, 5) if config['metric'] == 'minkowski' else 2\n",
        "      return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    if config['scale_data']:\n",
        "      scaler = StandardScaler()\n",
        "      X_tr = scaler.fit_transform(X_tr)\n",
        "      X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Initialize Model\n",
        "    model = KNeighborsClassifier(\n",
        "      n_neighbors=config['n_neighbors'],\n",
        "      weights=config['weights'],\n",
        "      metric=config['metric'],\n",
        "      p=config['p'],\n",
        "      algorithm=config['algorithm'],\n",
        "      n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]\n"
      ],
      "metadata": {
        "id": "R32Vd38a4BcW"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg5Slv3PVSxr"
      },
      "source": [
        "## Mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "dICXWfrFsyn5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "device = 'cuda'\n",
        "\n",
        "def create_model(config, input_size):\n",
        "    layers = []\n",
        "    in_features = input_size\n",
        "    activation_layer = getattr(nn, config['activation'])\n",
        "\n",
        "    for out_features in config['layer_sizes']:\n",
        "        layers.append(nn.Linear(in_features, out_features))\n",
        "        if config['layer_norm']:\n",
        "            layers.append(nn.LayerNorm(out_features))\n",
        "        layers.append(activation_layer())\n",
        "\n",
        "        if config['dropout'] > 0:\n",
        "            layers.append(nn.Dropout(config['dropout']))\n",
        "        in_features = out_features\n",
        "\n",
        "    layers.append(nn.Linear(in_features, 1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class MlpModel(ModelClass):\n",
        "    @classmethod\n",
        "    def update_data_config(cls, config, trial):\n",
        "        # --- Architecture Params ---\n",
        "        config['n_layers'] = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        config['layer_norm'] = trial.suggest_categorical(\"layer_norm\", [True, False])\n",
        "        config['activation'] = trial.suggest_categorical(\"activation\", [\n",
        "            \"ReLU\", \"LeakyReLU\", \"GELU\", \"SiLU\", \"Mish\", \"Hardswish\", \"ELU\", \"Tanh\", \"Sigmoid\"\n",
        "        ])\n",
        "        config['dropout'] = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "\n",
        "        # --- Training Params ---\n",
        "        config['lr'] = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "        config['batch_size'] = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
        "        config['threshold'] = trial.suggest_float(\"threshold\", 0.1, 0.7)\n",
        "        config['weight_decay'] = trial.suggest_float(\"weight_decay\", 1e-8, 1e-1, log=True)\n",
        "\n",
        "        # --- Dynamic Layer Sizes ---\n",
        "        config['layer_sizes'] = [\n",
        "            trial.suggest_int(f\"n_units_l{i}\", 64, 512, step=64)\n",
        "            for i in range(config['n_layers'])\n",
        "        ]\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def evaluate_model_intern(cls, X_tr, X_val, y_tr, config):\n",
        "        pass # Not used but required by base class\n",
        "\n",
        "    @classmethod\n",
        "    def train_predict(cls, config):\n",
        "        # --- Setup Data for Cross-Validation ---\n",
        "        kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        all_fold_scores = []\n",
        "\n",
        "        # Get data using the shared loader\n",
        "        X_current = data_loader.get_data(config['data_type'], config)\n",
        "        y_train = data_loader.y_train\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_current, y_train)):\n",
        "            X_tr, X_val = X_current[train_idx], X_current[val_idx]\n",
        "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "            # --- SMOTE ---\n",
        "            if config['use_smote']:\n",
        "                from imblearn.over_sampling import SMOTE\n",
        "                try:\n",
        "                    smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                    X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "            # Prepare Tensors\n",
        "            X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(device)\n",
        "            y_tr_t = torch.tensor(y_tr.reshape(-1, 1), dtype=torch.float32).to(device)\n",
        "            X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "\n",
        "            train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
        "            train_dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "            # Build Model\n",
        "            model = create_model(config, input_size=X_current.shape[1]).to(device)\n",
        "\n",
        "            # Optimizer & Scheduler\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='max', factor=0.5, patience=3\n",
        "            )\n",
        "\n",
        "            fold_curve = []\n",
        "\n",
        "            # Training Loop\n",
        "            for epoch in range(EPOCHS):\n",
        "                model.train()\n",
        "                for X_b, y_b in train_dl:\n",
        "                    optimizer.zero_grad()\n",
        "                    out = model(X_b)\n",
        "                    loss = loss_fn(out, y_b)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_logits = model(X_val_t)\n",
        "                    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
        "                    val_preds = (val_probs > config['threshold']).astype(int)\n",
        "                    score = f1_score(y_val, val_preds, average='macro')\n",
        "\n",
        "                # Update Scheduler\n",
        "                scheduler.step(score)\n",
        "                fold_curve.append(score)\n",
        "\n",
        "            all_fold_scores.append(fold_curve)\n",
        "\n",
        "        all_fold_scores = np.array(all_fold_scores)\n",
        "        mean_curve = np.mean(all_fold_scores, axis=0)\n",
        "        final_score = np.max(mean_curve)\n",
        "\n",
        "        return final_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX0EI9eAVXjT"
      },
      "source": [
        "## Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "0rvpS3JFVGYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb15069-4c6f-4f0e-bf85-f9e454cf5e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Optimization for model: ext\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-03 01:44:57,125] A new study created in RDB with name: ext_study\n",
            "[I 2025-12-03 01:45:10,691] Trial 0 finished with value: 0.5864794953608744 and parameters: {'data_type': 'SVD', 'use_smote': False, 'threshold': 0.23906287362135661, 'use_Isolation type': True, 'use_Location': False, 'use_Isolation source': True, 'use_Testing standard': True, 'input_size_pca_svd': 64, 'n_estimators': 250, 'max_depth': 28, 'criterion': 'entropy', 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_samples': 0.7, 'min_impurity_decrease': 0.013988513867563102}. Best is trial 0 with value: 0.5864794953608744.\n",
            "[I 2025-12-03 01:46:16,230] Trial 1 finished with value: 0.5995485345325254 and parameters: {'data_type': 'CHI2', 'use_smote': False, 'threshold': 0.45779835358098153, 'use_Isolation type': True, 'use_Location': False, 'use_Isolation source': False, 'use_Testing standard': False, 'input_size_chi2': 130, 'chi2_threshold': 0.07868350736632683, 'n_estimators': 100, 'max_depth': 12, 'criterion': 'entropy', 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 0.5, 'bootstrap': True, 'class_weight': None, 'max_samples': 0.9, 'min_impurity_decrease': 0.05205835265609955}. Best is trial 1 with value: 0.5995485345325254.\n",
            "[I 2025-12-03 01:47:24,425] Trial 2 finished with value: 0.6634436891494491 and parameters: {'data_type': 'CHI2', 'use_smote': False, 'threshold': 0.33044647615576944, 'use_Isolation type': True, 'use_Location': True, 'use_Isolation source': False, 'use_Testing standard': False, 'input_size_chi2': 632, 'chi2_threshold': 0.010042625044243886, 'n_estimators': 350, 'max_depth': 15, 'criterion': 'gini', 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True, 'class_weight': None, 'max_samples': 0.7, 'min_impurity_decrease': 0.04765809489728245}. Best is trial 2 with value: 0.6634436891494491.\n",
            "[I 2025-12-03 01:48:01,098] Trial 3 finished with value: 0.6726497094316857 and parameters: {'data_type': 'CHI2', 'use_smote': True, 'threshold': 0.4642710920232701, 'use_Isolation type': True, 'use_Location': True, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_chi2': 1000, 'chi2_threshold': 0.049974572631778236, 'smote_k': 9, 'smote_ratio': 0.8320308397036102, 'n_estimators': 100, 'max_depth': 6, 'criterion': 'gini', 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_samples': None, 'min_impurity_decrease': 0.008802413932331943}. Best is trial 3 with value: 0.6726497094316857.\n",
            "[I 2025-12-03 01:48:50,762] Trial 4 finished with value: 0.6804262876802165 and parameters: {'data_type': 'CHI2', 'use_smote': False, 'threshold': 0.177637732060838, 'use_Isolation type': True, 'use_Location': False, 'use_Isolation source': True, 'use_Testing standard': True, 'input_size_chi2': 138, 'chi2_threshold': 0.023784511373963372, 'n_estimators': 400, 'max_depth': 13, 'criterion': 'entropy', 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_features': 1.0, 'bootstrap': True, 'class_weight': None, 'max_samples': 0.5, 'min_impurity_decrease': 0.06007817715522182}. Best is trial 4 with value: 0.6804262876802165.\n",
            "[I 2025-12-03 01:48:55,352] Trial 5 finished with value: 0.6541239114624464 and parameters: {'data_type': 'PCA', 'use_smote': False, 'threshold': 0.36695316481663, 'use_Isolation type': False, 'use_Location': True, 'use_Isolation source': False, 'use_Testing standard': False, 'input_size_pca_svd': 64, 'n_estimators': 100, 'max_depth': 10, 'criterion': 'log_loss', 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.5, 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': None, 'min_impurity_decrease': 0.022471995226106878}. Best is trial 4 with value: 0.6804262876802165.\n",
            "[I 2025-12-03 01:49:01,750] Trial 6 finished with value: 0.2101684952113226 and parameters: {'data_type': 'SVD', 'use_smote': False, 'threshold': 0.21134039884653627, 'use_Isolation type': False, 'use_Location': False, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_pca_svd': 256, 'n_estimators': 100, 'max_depth': 20, 'criterion': 'entropy', 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 0.5, 'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_samples': 0.7, 'min_impurity_decrease': 0.03184480735018247}. Best is trial 4 with value: 0.6804262876802165.\n",
            "[I 2025-12-03 01:50:24,958] Trial 7 finished with value: 0.6863275354819394 and parameters: {'data_type': 'CHI2', 'use_smote': False, 'threshold': 0.5421700210004492, 'use_Isolation type': True, 'use_Location': False, 'use_Isolation source': False, 'use_Testing standard': True, 'input_size_chi2': 73, 'chi2_threshold': 0.00263677442260871, 'n_estimators': 450, 'max_depth': 14, 'criterion': 'gini', 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': None, 'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_samples': 0.5, 'min_impurity_decrease': 0.048800886498186584}. Best is trial 7 with value: 0.6863275354819394.\n",
            "[I 2025-12-03 01:50:42,425] Trial 8 finished with value: 0.22035621647027948 and parameters: {'data_type': 'PCA', 'use_smote': True, 'threshold': 0.10835741868233986, 'use_Isolation type': False, 'use_Location': True, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_pca_svd': 96, 'smote_k': 7, 'smote_ratio': 0.3205116933254209, 'n_estimators': 450, 'max_depth': 10, 'criterion': 'log_loss', 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 0.5, 'bootstrap': True, 'class_weight': None, 'max_samples': None, 'min_impurity_decrease': 0.0896415335830148}. Best is trial 7 with value: 0.6863275354819394.\n",
            "[I 2025-12-03 01:50:47,232] Trial 9 finished with value: 0.4621352145206882 and parameters: {'data_type': 'SVD', 'use_smote': True, 'threshold': 0.6851278447948451, 'use_Isolation type': False, 'use_Location': True, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_pca_svd': 512, 'smote_k': 4, 'smote_ratio': 0.8669879801048725, 'n_estimators': 100, 'max_depth': 16, 'criterion': 'log_loss', 'min_samples_split': 20, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': None, 'min_impurity_decrease': 0.059921971678743746}. Best is trial 7 with value: 0.6863275354819394.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = 'ext'\n",
        "    DB_FILE = f'sqlite:///{file_path}/Optuna/optuna_{model}.db'\n",
        "\n",
        "    print(f\"Starting Optimization for model: {model}\")\n",
        "\n",
        "    study_names = {\n",
        "        'knn': 'knn_study',\n",
        "        'mlp': 'mlp_study' ,\n",
        "        'reg': 'reg_study',\n",
        "        'xgb': 'xgb_study',\n",
        "        'for': 'for_study',\n",
        "        'ext': 'ext_study',\n",
        "    }\n",
        "\n",
        "    objectives = {\n",
        "        'knn': KnnModel.objective,\n",
        "        'mlp': MlpModel.objective,\n",
        "        'reg': LogisticRegressionModel.objective,\n",
        "        'xgb': XGBoostModel.objective,\n",
        "        'for': RandomForestModel.objective,\n",
        "        'ext': ExtraTreesModel.objective\n",
        "    }\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        study_name=study_names[model],\n",
        "        storage=DB_FILE,\n",
        "        direction=\"maximize\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objectives[model], n_trials=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "p9YQrbw2tB0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "id": "DEmyXVFytDJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c34009-7a8d-4246-f46c-35bcae0ddbb2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.visualization import (\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances,\n",
        "    plot_slice,\n",
        "    plot_contour,\n",
        "    plot_parallel_coordinate\n",
        ")\n",
        "\n",
        "print(study_names[model])\n",
        "study = optuna.load_study(study_name=study_names[model], storage=DB_FILE)\n",
        "\n",
        "print(f\"\\nBest F1 Score (Macro): {study.best_value:.4f}\")\n",
        "print(\"Best Config Found:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# fig1 = plot_optimization_history(study)\n",
        "# fig1.show()\n",
        "\n",
        "# fig2 = plot_param_importances(study)\n",
        "# fig2.show()\n",
        "\n",
        "# fig3 = plot_slice(study)\n",
        "# fig3.show()\n",
        "\n",
        "# fig4 = plot_parallel_coordinate(study)\n",
        "# fig4.show()"
      ],
      "metadata": {
        "id": "W9h59LQEtG68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cb5c2b-0559-4a82-d6e0-48607596f774"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knn_study\n",
            "\n",
            "Best F1 Score (Macro): 0.8140\n",
            "Best Config Found:\n",
            "  data_type: SVD\n",
            "  use_smote: True\n",
            "  threshold: 0.5229996009368388\n",
            "  use_Isolation type: False\n",
            "  use_Location: True\n",
            "  use_Isolation source: True\n",
            "  use_Testing standard: True\n",
            "  input_size_pca_svd: 128\n",
            "  smote_k: 3\n",
            "  smote_ratio: 0.3345491559902423\n",
            "  n_neighbors: 50\n",
            "  weights: distance\n",
            "  metric: cosine\n",
            "  scale_data: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "dIZHAnbrzDTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    model_name = 'xgb'\n",
        "\n",
        "    model_classes = {\n",
        "      'knn': KnnModel,\n",
        "      'mlp': MlpModel,\n",
        "      'reg': LogisticRegressionModel,\n",
        "      'xgb': XGBoostModel,\n",
        "      'for': RandomForestModel,\n",
        "    }\n",
        "\n",
        "    TargetClass = model_classes[model_name]\n",
        "\n",
        "    db_file = f'sqlite:///{file_path}/Optuna/optuna_{model_name}.db'\n",
        "    study_name = study_names[model_name]\n",
        "\n",
        "    print(f\"--- Loading Study: {study_name} ---\")\n",
        "    study = optuna.load_study(study_name=study_name, storage=db_file)\n",
        "    best_trial = study.best_trial\n",
        "\n",
        "    print(f\"Best CV F1 Score found: {best_trial.value:.4f}\")\n",
        "\n",
        "    # Reconstruct the full configuration dictionary using FixedTrial\n",
        "    fixed_trial = optuna.trial.FixedTrial(best_trial.params)\n",
        "    best_config = TargetClass.create_data_config(fixed_trial)\n",
        "\n",
        "    print(f\"\\n--- Training {TargetClass.__name__} on Full Data and Predicting ---\")\n",
        "\n",
        "    # Generate class predictions and save them.\n",
        "    # The 'predict' method handles all data preparation, training, and saving.\n",
        "    final_preds = TargetClass.predict(best_config, proba=False, save=True)\n",
        "\n",
        "    print(f\"\\nPrediction complete. Total test samples predicted: {len(final_preds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D83WjOnvzEAC",
        "outputId": "068e4cb5-aad3-48e4-e983-39ab14bd5bc7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Study: xgb_study ---\n",
            "Best CV F1 Score found: 0.8294\n",
            "\n",
            "--- Training XGBoostModel on Full Data and Predicting ---\n",
            "Saved submission to Submission_XGBoost_20251203_0135_PREDS.csv\n",
            "\n",
            "Prediction complete. Total test samples predicted: 1092\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FakbV7Im3_1n"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMmr1ZWTtdo5R14LqbYJY6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}