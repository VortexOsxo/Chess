{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VortexOsxo/Chess/blob/master/God_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDcP1v2sLxN"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k8x2miYsRpM",
        "outputId": "7dd1e4c4-ba80-41db-d873-1ac1daba7534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/Inf8245/'\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "NUM_FOLDS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P1qOaetVsOQu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_meta_data():\n",
        "    meta_train = pd.read_csv(f'{file_path}Data/metadata_train.csv')\n",
        "    meta_test = pd.read_csv(f'{file_path}Data/metadata_test.csv')\n",
        "    return meta_train, meta_test\n",
        "\n",
        "def load_data():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    X_train, y_train = data_train[\"X_train\"], data_train[\"y_train\"]\n",
        "    X_test = data_test[\"X_test\"]\n",
        "\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "def load_ids():\n",
        "    data_train = np.load(f'{file_path}Data/train.npz')\n",
        "    data_test = np.load(f'{file_path}Data/test.npz')\n",
        "\n",
        "    return data_train[\"ids\"], data_test[\"ids\"]\n",
        "\n",
        "def save_predictions(preds, filename):\n",
        "    _, test_ids = load_ids()\n",
        "\n",
        "    if len(test_ids) != len(preds):\n",
        "        raise ValueError(\"Length of test_ids and preds must be the same.\")\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"id\": test_ids.astype(str),\n",
        "        \"label\": preds\n",
        "    })\n",
        "\n",
        "    df.to_csv(f'{file_path}Predictions/{filename}.csv', index=False)\n",
        "\n",
        "def merge_train_test(X_train, X_test, debug=False):\n",
        "    X_combined = np.concatenate([X_train, X_test], axis=0)\n",
        "    if debug: print(X_combined.shape)\n",
        "    return X_combined\n",
        "\n",
        "def remove_null_variance_column(X_train, X_test, debug=False):\n",
        "    mask = (X_train.min(axis=0) != X_train.max(axis=0))\n",
        "\n",
        "    if debug: print(f'Initial Column number: {X_train.shape[1]}')\n",
        "    X_train = X_train[:, mask]\n",
        "    X_test  = X_test[:, mask]\n",
        "    if debug: print(f'After removed null variance: {X_train.shape[1]}')\n",
        "\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-Jc-wNytx4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxUsQWG-y0OJ"
      },
      "source": [
        "## PCA & SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gNA4otlBteYM"
      },
      "outputs": [],
      "source": [
        "def load_transform(transform, n_components, is_combined=True, tolerance=1e-5):\n",
        "  name = f'nc_{n_components}_combined_{str(is_combined).lower()}_tol_{tolerance}'\n",
        "  X_train = np.load(f'{file_path}Data/{transform}/{name}train.npy')\n",
        "  X_test = np.load(f'{file_path}Data/{transform}/{name}test.npy')\n",
        "  return X_train, X_test\n",
        "\n",
        "def load_pca(n_components, is_combined=True, tolerance=1e-5):\n",
        "  return load_transform('PCA', n_components, is_combined, tolerance)\n",
        "\n",
        "def load_svd(n_components, is_combined=True, tolerance=1e-5):\n",
        "  return load_transform('SVD', n_components, is_combined, tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOganBvY2I-j"
      },
      "source": [
        "## Chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x_pqcwUn7eqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
        "\n",
        "def load_var_chi2(X_train, X_test, y_train, n_components, threshold):\n",
        "  selector = VarianceThreshold(threshold=threshold)\n",
        "  X_train  =  selector.fit_transform(X_train)\n",
        "  X_test   = selector.transform(X_test)\n",
        "\n",
        "  selector = SelectKBest(chi2, k=n_components)\n",
        "\n",
        "  X_train = selector.fit_transform(X_train, y_train)\n",
        "  X_test  = selector.transform(X_test)\n",
        "\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBAoAxnH-wk_"
      },
      "source": [
        "## Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nWCX8kGn-zmV"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def save_metadata(meta_train, meta_test, reduced=False):\n",
        "  joblib.dump(meta_train, f\"{file_path}Data/X_meta_train_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "  joblib.dump(meta_test,  f\"{file_path}Data/X_meta_test_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "\n",
        "def load_meta_onehot(reduced=False, debug=False):\n",
        "  X_meta_train = joblib.load(f\"{file_path}Data/X_meta_train_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "  X_meta_test  = joblib.load(f\"{file_path}Data/X_meta_test_sparse{\"_reduced\" if reduced else \"\"}.pkl\")\n",
        "\n",
        "  X_meta_train_dense = X_meta_train.toarray() if sparse.issparse(X_meta_train) else X_meta_train\n",
        "  X_meta_test_dense  = X_meta_test.toarray()  if sparse.issparse(X_meta_test)  else X_meta_test\n",
        "\n",
        "  return X_meta_train_dense, X_meta_test_dense\n",
        "\n",
        "def append_meta_to_data(X_train, X_test, reduced=False, debug=False):\n",
        "  meta_train, meta_test = load_meta_onehot(reduced)\n",
        "\n",
        "  X_train_combined = np.hstack([meta_train, X_train])\n",
        "  X_test_combined  = np.hstack([meta_test,  X_test])\n",
        "\n",
        "  return X_train_combined, X_test_combined\n",
        "\n",
        "def build_categorical_features(meta_train, meta_test, selected_columns):\n",
        "  if len(selected_columns) == 0:\n",
        "    # No metadata selected by Optuna\n",
        "    n_train = meta_train.shape[0]\n",
        "    n_test = meta_test.shape[0]\n",
        "    return np.zeros((n_train, 0)), np.zeros((n_test, 0))\n",
        "\n",
        "  train_cat = meta_train[selected_columns].copy()\n",
        "  test_cat = meta_test[selected_columns].copy()\n",
        "\n",
        "  threshold = 15\n",
        "\n",
        "  for col in selected_columns:\n",
        "    counts = train_cat[col].value_counts(dropna=False)\n",
        "    keep_categories = set(counts[counts >= threshold].index)\n",
        "\n",
        "    def map_value(v):\n",
        "      return v if v in keep_categories else \"Other\"\n",
        "\n",
        "    train_cat[col] = train_cat[col].map(map_value)\n",
        "    test_cat[col] = test_cat[col].map(map_value)\n",
        "\n",
        "  encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "  train_encoded = encoder.fit_transform(train_cat)\n",
        "  test_encoded = encoder.transform(test_cat)\n",
        "\n",
        "  return train_encoded, test_encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ocuFbltfFv"
      },
      "source": [
        "# Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjGkyVozucKG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h-YR7Mtvudh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6321f3-d599-4560-a14d-1df705c64a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr00udAc0dLN"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KxEiSYcO0ecD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36a8782-a26e-4592-f492-11084ee3a74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [OK] Loaded input_size=64  | Shape: (1939, 64)\n",
            "  [OK] Loaded input_size=96  | Shape: (1939, 96)\n",
            "  [OK] Loaded input_size=128 | Shape: (1939, 128)\n",
            "  [OK] Loaded input_size=192 | Shape: (1939, 192)\n",
            "  [OK] Loaded input_size=256 | Shape: (1939, 256)\n",
            "  [OK] Loaded input_size=512 | Shape: (1939, 512)\n"
          ]
        }
      ],
      "source": [
        "SIZES = [64, 96, 128, 192, 256, 512]\n",
        "\n",
        "\n",
        "class MyDataLoader():\n",
        "    def __init__(self):\n",
        "        self.meta_train, self.meta_test = load_meta_data()\n",
        "        self.X_train, self.y_train, self.X_test = load_data()\n",
        "        self.data_dict = {\n",
        "            'PCA': {}, 'SVD':{},\n",
        "        }\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        for size in SIZES:\n",
        "            X_tr, _ = load_svd(n_components=size)\n",
        "            self.data_dict['SVD'][size] = X_tr\n",
        "\n",
        "            X_tr, _ = load_pca(n_components=size)\n",
        "            self.data_dict['PCA'][size] = X_tr\n",
        "            print(f\"  [OK] Loaded input_size={size:<3} | Shape: {X_tr.shape}\")\n",
        "\n",
        "\n",
        "    def get_combined_data(self, config, train, test=None) -> np.ndarray:\n",
        "        meta_train, meta_test = build_categorical_features(\n",
        "            self.meta_train, self.meta_test,\n",
        "            config['selected_columns'],\n",
        "        )\n",
        "        if test is None:\n",
        "            return np.hstack([meta_train, train])\n",
        "        return np.hstack([meta_train, train]), np.hstack([meta_test, test])\n",
        "\n",
        "\n",
        "    def get_val_data(self, type, config):\n",
        "        if type in ['PCA', 'SVD']:\n",
        "            X_train = self.data_dict[type][config['input_size']]\n",
        "        elif type == 'CHI2':\n",
        "            X_train, _ = load_var_chi2(self.X_train, self.X_test, self.y_train, config['input_size'], config['chi2_threshold'])\n",
        "        return self.get_combined_data(config, X_train)\n",
        "\n",
        "\n",
        "    def get_test_data(self, type, config):\n",
        "        if type == 'SVD':\n",
        "            X_train, X_test = load_svd(n_components=config['input_size'])\n",
        "        elif type == 'PCA':\n",
        "            X_train, X_test = load_pca(n_components=config['input_size'])\n",
        "        elif type == 'CHI2':\n",
        "            X_train, X_test = load_var_chi2(self.X_train, self.X_test, self.y_train, config['input_size'], config['chi2_threshold'])\n",
        "        return self.get_combined_data(config, X_train, X_test)\n",
        "\n",
        "\n",
        "data_loader = MyDataLoader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWnjEqHue5g"
      },
      "source": [
        "# Hyperparameter Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BaseModelClass"
      ],
      "metadata": {
        "id": "sE32dpvjHAcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractclassmethod\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import datetime\n",
        "\n",
        "class ModelClass():\n",
        "  @abstractclassmethod\n",
        "  def get_name(cls):\n",
        "    pass\n",
        "\n",
        "  @classmethod\n",
        "  def objective(cls, trial):\n",
        "    config = cls.create_data_config(trial)\n",
        "    return cls.evaluate_model(config)\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    pass\n",
        "\n",
        "  @classmethod\n",
        "  def create_data_config(cls, trial):\n",
        "    config = {\n",
        "        'data_type':    trial.suggest_categorical(\"data_type\", ['PCA', 'SVD', 'CHI2']),\n",
        "        'use_smote':    trial.suggest_categorical(\"use_smote\", [True, False]),\n",
        "        'threshold':    trial.suggest_float(\"threshold\", 0.1, 0.7),\n",
        "    }\n",
        "\n",
        "    # Metadata feature used\n",
        "    config['selected_columns'] = [\n",
        "        col for col in ['Isolation type', 'Location', 'Isolation source', 'Testing standard']\n",
        "        if trial.suggest_categorical(f\"use_{col}\", [True, False])\n",
        "    ]\n",
        "\n",
        "    # Conditionnal\n",
        "    config['chi2_threshold'] = None\n",
        "    if config['data_type'] == 'CHI2':\n",
        "      config['input_size'] = trial.suggest_int(\"input_size_chi2\", 64, 50000, log=True)\n",
        "      config['chi2_threshold'] = trial.suggest_float(\"chi2_threshold\", 0.001, 0.1)\n",
        "    else:\n",
        "      config['input_size'] = trial.suggest_categorical(\"input_size_pca_svd\", SIZES)\n",
        "\n",
        "    # Smote\n",
        "    config['smote_k'] = trial.suggest_int(\"smote_k\", 1, 10) if config['use_smote'] else None\n",
        "    config['smote_ratio'] = trial.suggest_float(\"smote_ratio\", 0.3, 1.0) if config['use_smote'] else None\n",
        "    return cls.update_data_config(config, trial)\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    pass\n",
        "\n",
        "  @classmethod\n",
        "  def evaluate_model(cls, config):\n",
        "    # Cross validation folds\n",
        "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    f1_scores = []\n",
        "\n",
        "    # Get Data\n",
        "    X_source = data_loader.get_val_data(config['data_type'], config)\n",
        "    y_source = data_loader.y_train\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_source, y_source)):\n",
        "        X_tr, X_val = X_source[train_idx], X_source[val_idx]\n",
        "        y_tr, y_val = y_source[train_idx], y_source[val_idx]\n",
        "\n",
        "        # --- SMOTE ---\n",
        "        if config['use_smote']:\n",
        "            try:\n",
        "                smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "            except ValueError:\n",
        "                print('Smote Failed...')\n",
        "                pass\n",
        "\n",
        "        # Function to evaluate a model, return its predictions\n",
        "        val_probs = cls.train_predict(X_tr, X_val, y_tr, config)\n",
        "\n",
        "        val_preds = (val_probs > config['threshold']).astype(int)\n",
        "        score = f1_score(y_val, val_preds, average='macro')\n",
        "        f1_scores.append(score)\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "  @classmethod\n",
        "  def predict(cls, config, proba=False, save=False):\n",
        "        X_train_full, X_test_final = data_loader.get_test_data(config['data_type'], config)\n",
        "        y_train_full = data_loader.y_train\n",
        "\n",
        "        # 2. --- SMOTE ---\n",
        "        if config['use_smote']:\n",
        "            print(\"Applying SMOTE to full training set...\")\n",
        "            try:\n",
        "                smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
        "            except ValueError:\n",
        "                print('Smote Failed on full set, proceeding without oversampling.')\n",
        "                pass\n",
        "\n",
        "        probs = cls.train_predict(X_train_full, X_test_final, y_train_full, config)\n",
        "        results = probs if proba else (probs > config['threshold']).astype(int)\n",
        "\n",
        "        if len(results.shape) > 1:\n",
        "             results = results.flatten()\n",
        "        if save:\n",
        "            model_name = cls.__name__.replace('Model', '')\n",
        "            date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "            pred_type = \"PROBS\" if proba else \"PREDS\"\n",
        "\n",
        "            filename = f\"Submission_{model_name}_{date_str}_{pred_type}\"\n",
        "            save_predictions(results, filename)\n",
        "            print(f\"Saved submission to {filename}.csv\")\n",
        "        return results\n",
        "\n",
        "  @classmethod\n",
        "  def save_oof_predictions(cls, config, model_name):\n",
        "      X_source = data_loader.get_val_data(config['data_type'], config)\n",
        "      y_source = data_loader.y_train\n",
        "\n",
        "      X_train_full, X_test_full = data_loader.get_test_data(config['data_type'], config)\n",
        "\n",
        "      oof_preds = np.zeros(X_source.shape[0])\n",
        "\n",
        "      kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "      for fold, (train_idx, val_idx) in enumerate(kf.split(X_source, y_source)):\n",
        "          X_tr, X_val = X_source[train_idx], X_source[val_idx]\n",
        "          y_tr = y_source[train_idx]\n",
        "\n",
        "          if config['use_smote']:\n",
        "              try:\n",
        "                  smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                  X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "              except ValueError:\n",
        "                  pass\n",
        "\n",
        "          fold_probs = cls.train_predict(X_tr, X_val, y_tr, config)\n",
        "          oof_preds[val_idx] = fold_probs\n",
        "\n",
        "      X_train_final, y_train_final = X_train_full, y_source\n",
        "      if config['use_smote']:\n",
        "          try:\n",
        "              smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "              X_train_final, y_train_final = smote.fit_resample(X_train_final, y_train_final)\n",
        "          except ValueError:\n",
        "              pass\n",
        "\n",
        "      # On utilise train_predict en considérant (Train Full -> Test Full) comme (Train -> Val)\n",
        "      test_probs = cls.train_predict(X_train_final, X_test_full, y_train_final, config)\n",
        "\n",
        "      # 4. Sauvegarde\n",
        "      import os\n",
        "      save_dir = f'{file_path}Data/OOF/'\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "      np.save(f'{save_dir}{model_name}_train.npy', oof_preds)\n",
        "      np.save(f'{save_dir}{model_name}_test.npy', test_probs)\n",
        "      print(f\"  [OK] Saved to {save_dir}{model_name}_*.npy\")"
      ],
      "metadata": {
        "id": "Nh_FXuD8HCsV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctMhNe_rX_ZH"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uuHWxZQbYBI2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class LogisticRegressionModel(ModelClass):\n",
        "  @classmethod\n",
        "  def get_name(cls):\n",
        "    return 'for'\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- LR Model Params ---\n",
        "    config['C'] = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
        "    config['penalty'] = trial.suggest_categorical(\"penalty\", ['l1', 'l2'])\n",
        "\n",
        "    # Solver choice depends on the penalty (saga supports both l1 and l2)\n",
        "    config['solver'] = 'saga'\n",
        "    config['max_iter'] = trial.suggest_int(\"max_iter\", 5000, 10000)\n",
        "\n",
        "    # --- Validation Params ---\n",
        "    config['scale_data'] = trial.suggest_categorical(\"scale_data\", [True, False])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    if config['scale_data']:\n",
        "      scaler = StandardScaler()\n",
        "      X_tr = scaler.fit_transform(X_tr)\n",
        "      X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Initialize and Train the Model\n",
        "    model = LogisticRegression(\n",
        "      C=config['C'],\n",
        "      penalty=config['penalty'],\n",
        "      solver=config['solver'],\n",
        "      max_iter=config['max_iter'],\n",
        "      random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwbbxAwZj4nZ"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tnww3UKej59v"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class RandomForestModel(ModelClass):\n",
        "  @classmethod\n",
        "  def get_name(cls):\n",
        "    return 'for'\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- RF Model Params ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 5, 30, log=True)\n",
        "    config['criterion']         = trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss'])\n",
        "    config['min_samples_split'] = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    config['min_samples_leaf']  = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "    config['max_features']      = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', 0.5, 0.7, 1.0, None])\n",
        "    config['bootstrap']         = trial.suggest_categorical(\"bootstrap\", [True])\n",
        "    config['class_weight']      = trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
        "    config['max_samples']       = trial.suggest_categorical(\"max_samples\", [None, 0.5, 0.7, 0.9])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "      model = RandomForestClassifier(\n",
        "        n_estimators=config['n_estimators'],\n",
        "        max_depth=config['max_depth'],\n",
        "        criterion=config['criterion'],\n",
        "        min_samples_split=config['min_samples_split'],\n",
        "        min_samples_leaf=config['min_samples_leaf'],\n",
        "        max_features=config['max_features'],\n",
        "        bootstrap=config['bootstrap'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=config['class_weight'],\n",
        "        max_samples=config['max_samples'],\n",
        "        n_jobs=-1\n",
        "      )\n",
        "\n",
        "      model.fit(X_tr, y_tr)\n",
        "      return model.predict_proba(X_val)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extremly Random Forest\n"
      ],
      "metadata": {
        "id": "zJXvBuL_3-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# Note: You can reuse the Random Forest's update_data_config method\n",
        "# or copy it here, as the parameters are nearly identical.\n",
        "\n",
        "class ExtraTreesModel(ModelClass):\n",
        "  @classmethod\n",
        "  def get_name(cls):\n",
        "    return 'ext',\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- ET Model Params (Copied from Random Forest, they are compatible) ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 5, 30, log=True)\n",
        "    config['criterion']         = trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss'])\n",
        "    config['min_samples_split'] = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    config['min_samples_leaf']  = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "    config['max_features']      = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', 0.5, 0.7, 1.0, None])\n",
        "    config['bootstrap']         = trial.suggest_categorical(\"bootstrap\", [True])\n",
        "    config['class_weight']      = trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
        "    config['max_samples']       = trial.suggest_categorical(\"max_samples\", [None, 0.5, 0.7, 0.9])\n",
        "\n",
        "    # ExtraTrees specific parameter, usually kept at 0 or tuned\n",
        "    config['min_impurity_decrease'] = trial.suggest_float(\"min_impurity_decrease\", 0.0, 0.1)\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "      model = ExtraTreesClassifier(\n",
        "        n_estimators=config['n_estimators'],\n",
        "        max_depth=config['max_depth'],\n",
        "        criterion=config['criterion'],\n",
        "        min_samples_split=config['min_samples_split'],\n",
        "        min_samples_leaf=config['min_samples_leaf'],\n",
        "        max_features=config['max_features'],\n",
        "        bootstrap=config['bootstrap'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=config['class_weight'],\n",
        "        max_samples=config['max_samples'],\n",
        "        min_impurity_decrease=config['min_impurity_decrease'], # Added ET specific parameter\n",
        "        n_jobs=-1\n",
        "      )\n",
        "\n",
        "      model.fit(X_tr, y_tr)\n",
        "      return model.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "biV5MYQ_4BU-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "V9zf_FvOZisb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "class XGBoostModel(ModelClass):\n",
        "  @classmethod\n",
        "  def get_name(cls):\n",
        "    return 'xgb'\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "    # --- XGBoost Model Params ---\n",
        "    config['n_estimators']      = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
        "    config['learning_rate']     = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
        "    config['max_depth']         = trial.suggest_int(\"max_depth\", 3, 15)\n",
        "    config['min_child_weight']  = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
        "    config['subsample']         = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
        "    config['colsample_bytree']  = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
        "    config['gamma']             = trial.suggest_float(\"gamma\", 0, 5)\n",
        "    config['reg_alpha']         = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True)\n",
        "    config['reg_lambda']        = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True)\n",
        "    config['balance_strategy']  = trial.suggest_categorical(\"balance_strategy\", [\"None\", \"Balanced\"])\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    # Calculate scale_pos_weight dynamicallly if strategy is Balanced\n",
        "    scale_pos_weight = 1.0\n",
        "    if config['balance_strategy'] == \"Balanced\":\n",
        "      scale_pos_weight = (len(y_tr) - np.sum(y_tr)) / np.sum(y_tr)\n",
        "\n",
        "    # Initialize and Train XGBoost\n",
        "    model = XGBClassifier(\n",
        "      n_estimators=config['n_estimators'],\n",
        "      learning_rate=config['learning_rate'],\n",
        "      max_depth=config['max_depth'],\n",
        "      min_child_weight=config['min_child_weight'],\n",
        "      subsample=config['subsample'],\n",
        "      colsample_bytree=config['colsample_bytree'],\n",
        "      gamma=config['gamma'],\n",
        "      reg_alpha=config['reg_alpha'],\n",
        "      reg_lambda=config['reg_lambda'],\n",
        "      scale_pos_weight=scale_pos_weight,\n",
        "      random_state=RANDOM_STATE,\n",
        "      n_jobs=-1,\n",
        "      tree_method='hist',\n",
        "      objective='binary:logistic',\n",
        "      eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "ukJz6bbDZkA0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "FakbV7Im3_1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class KnnModel(ModelClass):\n",
        "  @classmethod\n",
        "  def get_name(cls):\n",
        "    return 'knn'\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "      # --- KNN Model Params ---\n",
        "      config['n_neighbors']  = trial.suggest_int(\"n_neighbors\", 3, 100)\n",
        "      config['weights']      = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n",
        "      config['metric']       = trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'cosine', 'minkowski'])\n",
        "      config['algorithm']    = 'auto'\n",
        "\n",
        "      # --- Validation Params ---\n",
        "      config['scale_data']   = trial.suggest_categorical(\"scale_data\", [True, False])\n",
        "\n",
        "      # Conditional params for minkowski\n",
        "      config['p'] = trial.suggest_int(\"p\", 1, 5) if config['metric'] == 'minkowski' else 2\n",
        "      return config\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "    if config['scale_data']:\n",
        "      scaler = StandardScaler()\n",
        "      X_tr = scaler.fit_transform(X_tr)\n",
        "      X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Initialize Model\n",
        "    model = KNeighborsClassifier(\n",
        "      n_neighbors=config['n_neighbors'],\n",
        "      weights=config['weights'],\n",
        "      metric=config['metric'],\n",
        "      p=config['p'],\n",
        "      algorithm=config['algorithm'],\n",
        "      n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return model.predict_proba(X_val)[:, 1]\n"
      ],
      "metadata": {
        "id": "R32Vd38a4BcW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg5Slv3PVSxr"
      },
      "source": [
        "## Mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dICXWfrFsyn5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "device = 'cuda'\n",
        "\n",
        "def create_model(config, input_size):\n",
        "    layers = []\n",
        "    in_features = input_size\n",
        "    activation_layer = getattr(nn, config['activation'])\n",
        "\n",
        "    for out_features in config['layer_sizes']:\n",
        "        layers.append(nn.Linear(in_features, out_features))\n",
        "        if config['layer_norm']:\n",
        "            layers.append(nn.LayerNorm(out_features))\n",
        "        layers.append(activation_layer())\n",
        "\n",
        "        if config['dropout'] > 0:\n",
        "            layers.append(nn.Dropout(config['dropout']))\n",
        "        in_features = out_features\n",
        "\n",
        "    layers.append(nn.Linear(in_features, 1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class MlpModel(ModelClass):\n",
        "    @classmethod\n",
        "    def get_name(cls):\n",
        "      return 'mlp'\n",
        "\n",
        "    @classmethod\n",
        "    def update_data_config(cls, config, trial):\n",
        "        # --- Architecture Params ---\n",
        "        config['n_layers'] = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        config['layer_norm'] = trial.suggest_categorical(\"layer_norm\", [True, False])\n",
        "        config['activation'] = trial.suggest_categorical(\"activation\", [\n",
        "            \"ReLU\", \"LeakyReLU\", \"GELU\", \"SiLU\", \"Mish\", \"Hardswish\", \"ELU\", \"Tanh\", \"Sigmoid\"\n",
        "        ])\n",
        "        config['dropout'] = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "\n",
        "        # --- Training Params ---\n",
        "        config['lr'] = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "        config['batch_size'] = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
        "        config['threshold'] = trial.suggest_float(\"threshold\", 0.1, 0.7)\n",
        "        config['weight_decay'] = trial.suggest_float(\"weight_decay\", 1e-8, 1e-1, log=True)\n",
        "\n",
        "        # --- Dynamic Layer Sizes ---\n",
        "        config['layer_sizes'] = [\n",
        "            trial.suggest_int(f\"n_units_l{i}\", 64, 512, step=64)\n",
        "            for i in range(config['n_layers'])\n",
        "        ]\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "        pass # Not used but required by base class\n",
        "\n",
        "    @classmethod\n",
        "    def evaluate_model(cls, config):\n",
        "        # --- Setup Data for Cross-Validation ---\n",
        "        kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        all_fold_scores = []\n",
        "\n",
        "        # Get data using the shared loader\n",
        "        X_current = data_loader.get_val_data(config['data_type'], config)\n",
        "        y_train = data_loader.y_train\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_current, y_train)):\n",
        "            X_tr, X_val = X_current[train_idx], X_current[val_idx]\n",
        "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "            # --- SMOTE ---\n",
        "            if config['use_smote']:\n",
        "                from imblearn.over_sampling import SMOTE\n",
        "                try:\n",
        "                    smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                    X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "            # Prepare Tensors\n",
        "            X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(device)\n",
        "            y_tr_t = torch.tensor(y_tr.reshape(-1, 1), dtype=torch.float32).to(device)\n",
        "            X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "\n",
        "            train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
        "            train_dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "            # Build Model\n",
        "            model = create_model(config, input_size=X_current.shape[1]).to(device)\n",
        "\n",
        "            # Optimizer & Scheduler\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='max', factor=0.5, patience=3\n",
        "            )\n",
        "\n",
        "            fold_curve = []\n",
        "\n",
        "            # Training Loop\n",
        "            for epoch in range(EPOCHS):\n",
        "                model.train()\n",
        "                for X_b, y_b in train_dl:\n",
        "                    optimizer.zero_grad()\n",
        "                    out = model(X_b)\n",
        "                    loss = loss_fn(out, y_b)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_logits = model(X_val_t)\n",
        "                    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
        "                    val_preds = (val_probs > config['threshold']).astype(int)\n",
        "                    score = f1_score(y_val, val_preds, average='macro')\n",
        "\n",
        "                # Update Scheduler\n",
        "                scheduler.step(score)\n",
        "                fold_curve.append(score)\n",
        "\n",
        "            all_fold_scores.append(fold_curve)\n",
        "\n",
        "        all_fold_scores = np.array(all_fold_scores)\n",
        "        mean_curve = np.mean(all_fold_scores, axis=0)\n",
        "        final_score = np.max(mean_curve)\n",
        "\n",
        "        return final_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking"
      ],
      "metadata": {
        "id": "FhKwxG-bta8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StackingModel(ModelClass):\n",
        "\n",
        "  @classmethod\n",
        "  def update_data_config(cls, config, trial):\n",
        "      # --- Sélection des modèles de base ---\n",
        "      # On choisit quels modèles inclure dans le stack\n",
        "      config['base_models'] = []\n",
        "      possible_models = ['knn', 'reg', 'for', 'ext', 'xgb'] # 'mlp' exclu pour rapidité, à ajouter si besoin\n",
        "\n",
        "      for name in possible_models:\n",
        "          if trial.suggest_categorical(f\"use_{name}\", [True, False]):\n",
        "              config['base_models'].append(name)\n",
        "\n",
        "      # Force au moins 2 modèles pour le stacking\n",
        "      if len(config['base_models']) < 2:\n",
        "           # On force l'ajout de 2 modèles par défaut si optuna n'en choisit aucun\n",
        "           config['base_models'] = ['reg', 'xgb']\n",
        "\n",
        "      # --- Configuration du Méta-Modèle (Niveau 1) ---\n",
        "      config['meta_model'] = trial.suggest_categorical(\"meta_model\", ['reg', 'xgb'])\n",
        "\n",
        "      # Hyperparamètres spécifiques au méta-modèle\n",
        "      if config['meta_model'] == 'reg':\n",
        "          config['meta_C'] = trial.suggest_float(\"meta_C\", 0.1, 10.0, log=True)\n",
        "      elif config['meta_model'] == 'xgb':\n",
        "          config['meta_n_estimators'] = trial.suggest_int(\"meta_n_estimators\", 50, 200)\n",
        "          config['meta_lr'] = trial.suggest_float(\"meta_lr\", 0.01, 0.2)\n",
        "\n",
        "      return config\n",
        "\n",
        "  @classmethod\n",
        "  def get_base_config(cls, model_name):\n",
        "      \"\"\"\n",
        "      Retourne la configuration optimale (fixe) pour chaque modèle de base.\n",
        "      Remplacez ces valeurs par vos 'best_params' trouvés précédemment.\n",
        "      \"\"\"\n",
        "      if model_name == 'reg':\n",
        "          return {'C': 1.0, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 2000, 'scale_data': True}\n",
        "      elif model_name == 'knn':\n",
        "          return {'n_neighbors': 20, 'weights': 'distance', 'metric': 'cosine', 'p': 2, 'scale_data': True, 'algorithm': 'auto'}\n",
        "      elif model_name == 'for':\n",
        "          return {'n_estimators': 200, 'max_depth': 15, 'criterion': 'gini', 'min_samples_split': 5, 'min_samples_leaf': 2,\n",
        "                  'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': 0.7}\n",
        "      elif model_name == 'ext':\n",
        "          return {'n_estimators': 200, 'max_depth': 15, 'criterion': 'gini', 'min_samples_split': 5, 'min_samples_leaf': 2,\n",
        "                  'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': 0.7, 'min_impurity_decrease': 0.0}\n",
        "      elif model_name == 'xgb':\n",
        "          return {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.8,\n",
        "                  'colsample_bytree': 0.8, 'gamma': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'balance_strategy': 'Balanced'}\n",
        "      # Ajoutez MLP si nécessaire (nécessite params d'architecture)\n",
        "      return {}\n",
        "\n",
        "  @classmethod\n",
        "  def get_model_class(cls, name):\n",
        "      \"\"\"Mappe le nom du modèle à sa classe\"\"\"\n",
        "      mapping = {\n",
        "          'reg': LogisticRegressionModel,\n",
        "          'knn': KnnModel,\n",
        "          'for': RandomForestModel,\n",
        "          'ext': ExtraTreesModel,\n",
        "          'xgb': XGBoostModel,\n",
        "          'mlp': MlpModel\n",
        "      }\n",
        "      return mapping[name]\n",
        "\n",
        "  @classmethod\n",
        "  def train_predict(cls, X_tr, X_val, y_tr, config):\n",
        "      \"\"\"\n",
        "      Cette fonction n'est PAS utilisée pour le stacking global,\n",
        "      car le stacking nécessite une logique plus complexe (OOF).\n",
        "      Laissez vide ou levez une erreur.\n",
        "      \"\"\"\n",
        "      raise NotImplementedError(\"Utilisez evaluate_model directement pour le Stacking.\")\n",
        "\n",
        "  @classmethod\n",
        "  def evaluate_model(cls, config):\n",
        "      # 1. Chargement des données globales\n",
        "      # Note: Tous les modèles de base partageront ces données (ex: PCA 128)\n",
        "      X_source = data_loader.get_val_data(config['data_type'], config)\n",
        "      y_source = data_loader.y_train\n",
        "\n",
        "      kf_outer = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "      f1_scores = []\n",
        "\n",
        "      print(f\"--- Eval Stacking [Base: {config['base_models']}] [Meta: {config['meta_model']}] ---\")\n",
        "\n",
        "      # BOUCLE EXTERNE : Validation du Stacking complet\n",
        "      for fold_idx, (train_idx, val_idx) in enumerate(kf_outer.split(X_source, y_source)):\n",
        "          X_train_outer, X_val_outer = X_source[train_idx], X_source[val_idx]\n",
        "          y_train_outer, y_val_outer = y_source[train_idx], y_source[val_idx]\n",
        "\n",
        "          # --- ÉTAPE 1 : Génération des Méta-Features (OOF) sur X_train_outer ---\n",
        "          # Nous avons besoin des prédictions des modèles de base sur X_train_outer\n",
        "          # pour entraîner le méta-modèle. Pour éviter la fuite, on utilise une CV interne.\n",
        "\n",
        "          n_samples = X_train_outer.shape[0]\n",
        "          n_models = len(config['base_models'])\n",
        "          oof_train = np.zeros((n_samples, n_models)) # Méta-features pour l'entraînement\n",
        "\n",
        "          kf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "          for inner_train_idx, inner_val_idx in kf_inner.split(X_train_outer, y_train_outer):\n",
        "              X_inner_tr = X_train_outer[inner_train_idx]\n",
        "              y_inner_tr = y_train_outer[inner_train_idx]\n",
        "              X_inner_val = X_train_outer[inner_val_idx]\n",
        "\n",
        "              # Appliquer SMOTE uniquement sur le fold d'entraînement interne\n",
        "              if config['use_smote']:\n",
        "                  try:\n",
        "                      smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                      X_inner_tr, y_inner_tr = smote.fit_resample(X_inner_tr, y_inner_tr)\n",
        "                  except ValueError:\n",
        "                      pass\n",
        "\n",
        "              # Entraîner chaque modèle de base et prédire sur le fold de validation interne\n",
        "              for i, name in enumerate(config['base_models']):\n",
        "                  BaseClass = cls.get_model_class(name)\n",
        "                  base_conf = cls.get_base_config(name)\n",
        "\n",
        "                  # Utilisation de VOTRE pipeline train_predict existant\n",
        "                  probs = BaseClass.train_predict(X_inner_tr, X_inner_val, y_inner_tr, base_conf)\n",
        "                  oof_train[inner_val_idx, i] = probs\n",
        "\n",
        "          # --- ÉTAPE 2 : Entraînement du Méta-Modèle ---\n",
        "          # Le méta-modèle apprend à partir des prédictions OOF\n",
        "          MetaClass = cls.get_model_class(config['meta_model'])\n",
        "\n",
        "          # Config du méta-modèle (construit à partir de la config optuna)\n",
        "          meta_conf = {'scale_data': False} # Pas de scaling nécessaire sur des probas\n",
        "          if config['meta_model'] == 'reg':\n",
        "              meta_conf.update({'C': config['meta_C'], 'penalty': 'l2', 'solver': 'saga', 'max_iter': 2000})\n",
        "          elif config['meta_model'] == 'xgb':\n",
        "              meta_conf.update({'n_estimators': config['meta_n_estimators'], 'learning_rate': config['meta_lr'],\n",
        "                                'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1.0,\n",
        "                                'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 0, 'balance_strategy': 'None'})\n",
        "\n",
        "          # On utilise train_predict du méta-modèle, mais attendez...\n",
        "          # Pour prédire sur X_val_outer, nous avons besoin des méta-features de X_val_outer !\n",
        "\n",
        "          # --- ÉTAPE 3 : Génération des Méta-Features pour la Validation (X_val_outer) ---\n",
        "          # On doit ré-entraîner les modèles de base sur TOUT X_train_outer et prédire X_val_outer\n",
        "          oof_test = np.zeros((X_val_outer.shape[0], n_models))\n",
        "\n",
        "          # SMOTE sur tout le train outer avant d'entraîner les modèles finaux de ce fold\n",
        "          X_train_outer_smoted, y_train_outer_smoted = X_train_outer, y_train_outer\n",
        "          if config['use_smote']:\n",
        "               try:\n",
        "                  smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                  X_train_outer_smoted, y_train_outer_smoted = smote.fit_resample(X_train_outer, y_train_outer)\n",
        "               except ValueError:\n",
        "                  pass\n",
        "\n",
        "          for i, name in enumerate(config['base_models']):\n",
        "              BaseClass = cls.get_model_class(name)\n",
        "              base_conf = cls.get_base_config(name)\n",
        "              # Entraînement sur tout le train outer -> Prédire sur val outer\n",
        "              probs = BaseClass.train_predict(X_train_outer_smoted, X_val_outer, y_train_outer_smoted, base_conf)\n",
        "              oof_test[:, i] = probs\n",
        "\n",
        "          # --- ÉTAPE 4 : Prédiction Finale et Score ---\n",
        "          # Le méta-modèle est entraîné sur oof_train et prédit sur oof_test\n",
        "          # On peut utiliser train_predict du Méta modèle directement ici\n",
        "          final_probs = MetaClass.train_predict(oof_train, oof_test, y_train_outer, meta_conf)\n",
        "\n",
        "          final_preds = (final_probs > config['threshold']).astype(int)\n",
        "          score = f1_score(y_val_outer, final_preds, average='macro')\n",
        "          f1_scores.append(score)\n",
        "\n",
        "      return np.mean(f1_scores)\n",
        "\n",
        "  @classmethod\n",
        "  def predict(cls, config, proba=False, save=False):\n",
        "      \"\"\"\n",
        "      Logique de prédiction finale pour la soumission.\n",
        "      Doit répéter la logique OOF sur l'ensemble du dataset d'entraînement.\n",
        "      \"\"\"\n",
        "      # 1. Données complètes\n",
        "      X_train_full, X_test_final = data_loader.get_test_data(config['data_type'], config)\n",
        "      y_train_full = data_loader.y_train\n",
        "\n",
        "      n_models = len(config['base_models'])\n",
        "\n",
        "      print(\"--- Stacking Prediction: Generating OOF features for Meta-Model ---\")\n",
        "\n",
        "      # 2. Générer les OOF sur le Train complet (pour entraîner le méta-modèle)\n",
        "      oof_train_full = np.zeros((X_train_full.shape[0], n_models))\n",
        "      kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "      for train_idx, val_idx in kf.split(X_train_full, y_train_full):\n",
        "          X_tr, y_tr = X_train_full[train_idx], y_train_full[train_idx]\n",
        "          X_val = X_train_full[val_idx]\n",
        "\n",
        "          if config['use_smote']:\n",
        "              try:\n",
        "                  smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "                  X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
        "              except ValueError: pass\n",
        "\n",
        "          for i, name in enumerate(config['base_models']):\n",
        "              BaseClass = cls.get_model_class(name)\n",
        "              base_conf = cls.get_base_config(name)\n",
        "              oof_train_full[val_idx, i] = BaseClass.train_predict(X_tr, X_val, y_tr, base_conf)\n",
        "\n",
        "      # 3. Générer les features pour le Test (en entraînant sur tout le Train)\n",
        "      print(\"--- Stacking Prediction: Generating features for Test Set ---\")\n",
        "      oof_test_final = np.zeros((X_test_final.shape[0], n_models))\n",
        "\n",
        "      # SMOTE final sur tout le dataset\n",
        "      X_train_smoted, y_train_smoted = X_train_full, y_train_full\n",
        "      if config['use_smote']:\n",
        "           try:\n",
        "              smote = SMOTE(k_neighbors=config['smote_k'], sampling_strategy=config['smote_ratio'], random_state=RANDOM_STATE)\n",
        "              X_train_smoted, y_train_smoted = smote.fit_resample(X_train_full, y_train_full)\n",
        "           except ValueError: pass\n",
        "\n",
        "      for i, name in enumerate(config['base_models']):\n",
        "          BaseClass = cls.get_model_class(name)\n",
        "          base_conf = cls.get_base_config(name)\n",
        "          oof_test_final[:, i] = BaseClass.train_predict(X_train_smoted, X_test_final, y_train_smoted, base_conf)\n",
        "\n",
        "      # 4. Entraîner le Méta-Modèle et Prédire\n",
        "      print(\"--- Stacking Prediction: Final Meta-Model Training ---\")\n",
        "      MetaClass = cls.get_model_class(config['meta_model'])\n",
        "\n",
        "      meta_conf = {'scale_data': False}\n",
        "      if config['meta_model'] == 'reg':\n",
        "          meta_conf.update({'C': config['meta_C'], 'penalty': 'l2', 'solver': 'saga', 'max_iter': 2000})\n",
        "      elif config['meta_model'] == 'xgb':\n",
        "          meta_conf.update({'n_estimators': config['meta_n_estimators'], 'learning_rate': config['meta_lr'],\n",
        "                            'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1.0,\n",
        "                            'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 0, 'balance_strategy': 'None'})\n",
        "\n",
        "      final_probs = MetaClass.train_predict(oof_train_full, oof_test_final, y_train_full, meta_conf)\n",
        "\n",
        "      results = final_probs if proba else (final_probs > config['threshold']).astype(int)\n",
        "\n",
        "      if save:\n",
        "            model_name = \"Stacking\"\n",
        "            date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "            filename = f\"Submission_{model_name}_{date_str}\"\n",
        "            save_predictions(results, filename)\n",
        "            print(f\"Saved submission to {filename}.csv\")\n",
        "\n",
        "      return results"
      ],
      "metadata": {
        "id": "2bCvr6p4tccn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX0EI9eAVXjT"
      },
      "source": [
        "## Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0rvpS3JFVGYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "outputId": "c2170e9b-18c3-4f98-d732-80bc0bfe8ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Optimization for model: RandomForestModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-03 17:35:11,403] Using an existing study with name 'for_study' instead of creating a new one.\n",
            "[I 2025-12-03 17:39:35,174] Trial 2307 finished with value: 0.771569025797711 and parameters: {'data_type': 'PCA', 'use_smote': True, 'threshold': 0.34986751276224576, 'use_Isolation type': False, 'use_Location': True, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_pca_svd': 256, 'smote_k': 10, 'smote_ratio': 0.3975616838099665, 'n_estimators': 300, 'max_depth': 22, 'criterion': 'entropy', 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': 0.9}. Best is trial 1434 with value: 0.831008984146942.\n",
            "[W 2025-12-03 17:39:41,965] Trial 2308 failed with parameters: {'data_type': 'SVD', 'use_smote': True, 'threshold': 0.3832849149627641, 'use_Isolation type': True, 'use_Location': True, 'use_Isolation source': True, 'use_Testing standard': False, 'input_size_pca_svd': 256, 'smote_k': 10, 'smote_ratio': 0.39472198837731626, 'n_estimators': 400, 'max_depth': 26, 'criterion': 'entropy', 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced', 'max_samples': None} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-703168580.py\", line 13, in objective\n",
            "    return cls.evaluate_model(config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-703168580.py\", line 74, in evaluate_model\n",
            "    val_probs = cls.train_predict(X_tr, X_val, y_tr, config)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1735846710.py\", line 39, in train_predict\n",
            "    return model.predict_proba(X_val)[:, 1]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\", line 957, in predict_proba\n",
            "    Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 2072, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1682, in _get_outputs\n",
            "    yield from self._retrieve()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1800, in _retrieve\n",
            "    time.sleep(0.01)\n",
            "KeyboardInterrupt\n",
            "[W 2025-12-03 17:39:41,974] Trial 2308 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-500971358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     ):\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-703168580.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(cls, trial)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_data_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabstractclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-703168580.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Function to evaluate a model, return its predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mval_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_probs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1735846710.py\u001b[0m in \u001b[0;36mtrain_predict\u001b[0;34m(cls, X_tr, X_val, y_tr, config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    955\u001b[0m         ]\n\u001b[1;32m    956\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = RandomForestModel\n",
        "    DB_FILE = f'sqlite:///{file_path}/Optuna/optuna_{model.get_name()}.db'\n",
        "    print(f\"Starting Optimization for model: {str(model.__name__)}\")\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        study_name=f'{model.get_name()}_study',\n",
        "        storage=DB_FILE,\n",
        "        direction=\"maximize\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(model.objective, n_trials=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "p9YQrbw2tB0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "id": "DEmyXVFytDJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.visualization import (\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances,\n",
        "    plot_slice,\n",
        "    plot_contour,\n",
        "    plot_parallel_coordinate\n",
        ")\n",
        "\n",
        "print(study_names[model])\n",
        "study = optuna.load_study(study_name=study_names[model], storage=DB_FILE)\n",
        "\n",
        "print(f\"\\nBest F1 Score (Macro): {study.best_value:.4f}\")\n",
        "print(\"Best Config Found:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig1.show()\n",
        "\n",
        "fig2 = plot_param_importances(study)\n",
        "fig2.show()\n",
        "\n",
        "fig3 = plot_slice(study)\n",
        "fig3.show()\n",
        "\n",
        "fig4 = plot_parallel_coordinate(study)\n",
        "fig4.show()"
      ],
      "metadata": {
        "id": "W9h59LQEtG68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking\n"
      ],
      "metadata": {
        "id": "4jagZjd_H5T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Out of folds"
      ],
      "metadata": {
        "id": "wnzvqXCqIZJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\" and True:\n",
        "    models = [\n",
        "      KnnModel,\n",
        "      LogisticRegressionModel,\n",
        "      XGBoostModel,\n",
        "      RandomForestModel,\n",
        "      ExtraTreesModel,\n",
        "      # 'mlp': MlpModel # Ajoutez si vous l'avez optimisé\n",
        "    ]\n",
        "\n",
        "    print(\"=== STARTING OOF GENERATION FOR ALL MODELS ===\")\n",
        "\n",
        "    for model in models:\n",
        "        try:\n",
        "            db_file = f'sqlite:///{file_path}/Optuna/optuna_{model.get_name()}.db'\n",
        "            study_name = f'{model.get_name()}_study'\n",
        "\n",
        "            # 1. Charger l'étude\n",
        "            study = optuna.load_study(study_name=study_name, storage=db_file)\n",
        "            best_trial = study.best_trial\n",
        "            print(f\"\\nProcessing {model.__name__} (Best Score: {best_trial.value:.4f})\")\n",
        "\n",
        "            # 2. Reconstruire la config\n",
        "            fixed_trial = optuna.trial.FixedTrial(best_trial.params)\n",
        "            best_config = model.create_data_config(fixed_trial)\n",
        "\n",
        "            # 3. Générer et Sauvegarder\n",
        "            model.save_oof_predictions(best_config, f'model')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {model}: {e}\")\n",
        "\n",
        "    print(\"\\n=== GENERATION COMPLETE ===\")"
      ],
      "metadata": {
        "id": "z5PHN0FaKuRM",
        "outputId": "1694474c-2a96-4f96-d3ff-16043d421433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STARTING OOF GENERATION FOR ALL MODELS ===\n",
            "\n",
            "Processing KnnModel (Best Score: 0.8140)\n",
            "  [OK] Saved to /content/drive/My Drive/Inf8245/Data/OOF/knn_*.npy\n",
            "\n",
            "Processing LogisticRegressionModel (Best Score: 0.8310)\n",
            "Skipping reg: The value of the parameter 'C' is not found. Please set it at the construction of the FixedTrial object.\n",
            "\n",
            "Processing XGBoostModel (Best Score: 0.8312)\n",
            "  [OK] Saved to /content/drive/My Drive/Inf8245/Data/OOF/xgb_*.npy\n",
            "\n",
            "Processing RandomForestModel (Best Score: 0.8310)\n",
            "  [OK] Saved to /content/drive/My Drive/Inf8245/Data/OOF/for_*.npy\n",
            "Skipping ext: 'Record does not exist.'\n",
            "\n",
            "=== GENERATION COMPLETE ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "dIZHAnbrzDTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    model = KnnModel\n",
        "\n",
        "    db_file = f'sqlite:///{file_path}/Optuna/optuna_{model}.db'\n",
        "\n",
        "    print(f\"--- Loading Study: {model.__name__} ---\")\n",
        "    study = optuna.load_study(study_name=model.get_study_namne(), storage=db_file)\n",
        "    best_trial = study.best_trial\n",
        "\n",
        "    print(f\"Best CV F1 Score found: {best_trial.value:.4f}\")\n",
        "\n",
        "    # Reconstruct the full configuration dictionary using FixedTrial\n",
        "    fixed_trial = optuna.trial.FixedTrial(best_trial.params)\n",
        "    best_config = model.create_data_config(fixed_trial)\n",
        "\n",
        "    print(f\"\\n--- Training {model.__name__} on Full Data and Predicting ---\")\n",
        "\n",
        "    # Generate class predictions and save them.\n",
        "    # The 'predict' method handles all data preparation, training, and saving.\n",
        "    final_preds = model.predict(best_config, proba=False, save=True)\n",
        "\n",
        "    print(f\"\\nPrediction complete. Total test samples predicted: {len(final_preds)}\")"
      ],
      "metadata": {
        "id": "D83WjOnvzEAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FakbV7Im3_1n"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMbzXKMdHu0oozk6SpPdT7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}